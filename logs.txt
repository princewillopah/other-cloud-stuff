
==> Audit <==
|---------|--------------------------------|--------------------|----------------|---------|---------------------|---------------------|
| Command |              Args              |      Profile       |      User      | Version |     Start Time      |      End Time       |
|---------|--------------------------------|--------------------|----------------|---------|---------------------|---------------------|
| start   | -p My-Main-Cluster             | My-Main-Cluster    | princewillopah | v1.33.0 | 02 May 24 17:20 UTC | 02 May 24 17:21 UTC |
| node    | add --worker -p                | My-Main-Cluster    | princewillopah | v1.33.0 | 02 May 24 18:42 UTC |                     |
|         | My-Main-Cluster                |                    |                |         |                     |                     |
| delete  |                                | minikube           | princewillopah | v1.33.0 | 02 May 24 19:25 UTC | 02 May 24 19:25 UTC |
| node    | add --worker -p                | My-Main-Cluster    | princewillopah | v1.33.0 | 03 May 24 03:23 UTC |                     |
|         | My-Main-Cluster                |                    |                |         |                     |                     |
| delete  | -p My-Main-Cluster             | My-Main-Cluster    | princewillopah | v1.33.0 | 03 May 24 03:35 UTC | 03 May 24 03:35 UTC |
| start   |                                | minikube           | princewillopah | v1.33.0 | 03 May 24 03:43 UTC | 03 May 24 03:44 UTC |
| node    | add --worker                   | minikube           | princewillopah | v1.33.0 | 03 May 24 03:47 UTC | 03 May 24 03:47 UTC |
| kubectl | cluster-info                   | minikube           | princewillopah | v1.33.0 | 03 May 24 03:58 UTC | 03 May 24 03:58 UTC |
| node    | add --worker                   | minikube           | princewillopah | v1.33.0 | 03 May 24 14:03 UTC | 03 May 24 14:04 UTC |
| node    | add --worker                   | minikube           | princewillopah | v1.33.0 | 03 May 24 14:05 UTC | 03 May 24 14:05 UTC |
| delete  |                                | minikube           | princewillopah | v1.33.0 | 03 May 24 16:42 UTC | 03 May 24 16:42 UTC |
| start   | --nodes 4 -p multinode-demo    | multinode-demo     | princewillopah | v1.33.0 | 03 May 24 17:41 UTC | 03 May 24 17:42 UTC |
| addons  | enable metrics-server          | minikube           | princewillopah | v1.33.0 | 05 May 24 21:53 UTC |                     |
| addons  | enable metrics-server -p       | multinode-demo     | princewillopah | v1.33.0 | 05 May 24 21:59 UTC | 05 May 24 21:59 UTC |
|         | multinode-demo                 |                    |                |         |                     |                     |
| node    | delete multinode-demo -p       | multinode-demo-m02 | princewillopah | v1.33.0 | 07 May 24 11:12 UTC |                     |
|         | multinode-demo-m02             |                    |                |         |                     |                     |
| node    | delete multinode-demo -p       | multinode-demo-m02 | princewillopah | v1.33.0 | 07 May 24 11:12 UTC |                     |
|         | multinode-demo-m02             |                    |                |         |                     |                     |
| node    | delete multinode-demo-m02 -p   | multinode-demo     | princewillopah | v1.33.0 | 07 May 24 11:13 UTC | 07 May 24 11:13 UTC |
|         | multinode-demo                 |                    |                |         |                     |                     |
| node    | delete multinode-demo-m04 -p   | multinode-demo     | princewillopah | v1.33.0 | 07 May 24 11:13 UTC | 07 May 24 11:13 UTC |
|         | multinode-demo                 |                    |                |         |                     |                     |
| addons  | list                           | minikube           | princewillopah | v1.33.0 | 07 May 24 11:14 UTC | 07 May 24 11:14 UTC |
| addons  | list                           | minikube           | princewillopah | v1.33.0 | 07 May 24 11:15 UTC | 07 May 24 11:15 UTC |
| node    | delete multinode-demo-m03 -p   | multinode-demo     | princewillopah | v1.33.0 | 07 May 24 12:36 UTC | 07 May 24 12:36 UTC |
|         | multinode-demo                 |                    |                |         |                     |                     |
| service | mon                            | minikube           | princewillopah | v1.33.0 | 11 May 24 07:54 UTC |                     |
| service | mon -p multinode-demo          | multinode-demo     | princewillopah | v1.33.0 | 11 May 24 07:55 UTC | 11 May 24 07:55 UTC |
| service | mongodb -p multinode-demo      | multinode-demo     | princewillopah | v1.33.0 | 11 May 24 08:40 UTC | 11 May 24 08:40 UTC |
| ssh     | -p multinode-demo              | multinode-demo     | princewillopah | v1.33.0 | 12 May 24 10:34 UTC |                     |
| ip      |                                | minikube           | princewillopah | v1.33.0 | 23 May 24 09:13 UTC |                     |
| ip      | multinode-demo ip              | multinode-demo     | princewillopah | v1.33.0 | 23 May 24 09:13 UTC | 23 May 24 09:13 UTC |
| ip      |                                | minikube           | princewillopah | v1.33.0 | 23 May 24 22:06 UTC |                     |
| ip      | -p multinode-demo              | multinode-demo     | princewillopah | v1.33.0 | 23 May 24 22:06 UTC | 23 May 24 22:06 UTC |
| addons  | enable ingress -p              | multinode-demo     | princewillopah | v1.33.0 | 30 May 24 05:46 UTC |                     |
|         | multinode-demo                 |                    |                |         |                     |                     |
|---------|--------------------------------|--------------------|----------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/05/03 17:41:25
Running on machine: Main-VM
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0503 17:41:25.239717  687371 out.go:291] Setting OutFile to fd 1 ...
I0503 17:41:25.239876  687371 out.go:343] isatty.IsTerminal(1) = true
I0503 17:41:25.239881  687371 out.go:304] Setting ErrFile to fd 2...
I0503 17:41:25.239884  687371 out.go:343] isatty.IsTerminal(2) = true
I0503 17:41:25.240156  687371 root.go:338] Updating PATH: /home/princewillopah/.minikube/bin
I0503 17:41:25.240881  687371 out.go:298] Setting JSON to false
I0503 17:41:25.241697  687371 start.go:129] hostinfo: {"hostname":"Main-VM","uptime":92902,"bootTime":1714665183,"procs":111,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.0-67-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"guest","hostId":"0050268e-38b6-d2f5-079e-ae4a66339d5b"}
I0503 17:41:25.241757  687371 start.go:139] virtualization: kvm guest
I0503 17:41:25.282255  687371 out.go:177] üòÑ  [multinode-demo] minikube v1.33.0 on Ubuntu 22.04 (kvm/amd64)
I0503 17:41:25.309571  687371 notify.go:220] Checking for updates...
I0503 17:41:25.309857  687371 driver.go:392] Setting default libvirt URI to qemu:///system
I0503 17:41:25.309880  687371 global.go:112] Querying for installed drivers using PATH=/home/princewillopah/.minikube/bin:/home/princewillopah/.vscode-server/bin/da76f93349a72022ca4670c1b84860304616aaa2/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
I0503 17:41:25.321804  687371 global.go:133] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0503 17:41:25.321941  687371 global.go:133] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0503 17:41:25.321960  687371 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0503 17:41:25.322026  687371 global.go:133] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I0503 17:41:25.322082  687371 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0503 17:41:25.322140  687371 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0503 17:41:25.322176  687371 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0503 17:41:25.345514  687371 docker.go:122] docker version: linux-24.0.5:
I0503 17:41:25.345625  687371 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0503 17:41:25.370362  687371 info.go:266] docker info: {ID:dc9acb05-5068-4109-9932-81348402234b Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:35 SystemTime:2024-05-03 17:41:25.361244904 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-67-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:8336138240 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Main-VM Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0503 17:41:25.370435  687371 docker.go:295] overlay module found
I0503 17:41:25.370461  687371 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0503 17:41:25.370475  687371 driver.go:314] not recommending "none" due to default: false
I0503 17:41:25.370483  687371 driver.go:314] not recommending "ssh" due to default: false
I0503 17:41:25.370499  687371 driver.go:349] Picked: docker
I0503 17:41:25.370510  687371 driver.go:350] Alternatives: [none ssh]
I0503 17:41:25.370514  687371 driver.go:351] Rejects: [podman kvm2 qemu2 virtualbox vmware]
I0503 17:41:25.373399  687371 out.go:177] ‚ú®  Automatically selected the docker driver. Other choices: none, ssh
I0503 17:41:25.376109  687371 start.go:297] selected driver: docker
I0503 17:41:25.376122  687371 start.go:901] validating driver "docker" against <nil>
I0503 17:41:25.376134  687371 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0503 17:41:25.376250  687371 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0503 17:41:25.402065  687371 info.go:266] docker info: {ID:dc9acb05-5068-4109-9932-81348402234b Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:35 SystemTime:2024-05-03 17:41:25.392282716 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-67-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:8336138240 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Main-VM Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0503 17:41:25.402257  687371 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0503 17:41:25.402567  687371 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=7949MB, container=7949MB
I0503 17:41:25.402720  687371 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0503 17:41:25.404780  687371 out.go:177] üìå  Using Docker driver with root privileges
I0503 17:41:25.406148  687371 cni.go:84] Creating CNI manager for ""
I0503 17:41:25.406164  687371 cni.go:136] multinode detected (0 nodes found), recommending kindnet
I0503 17:41:25.406171  687371 start_flags.go:319] Found "CNI" CNI - setting NetworkPlugin=cni
I0503 17:41:25.406307  687371 start.go:340] cluster config:
{Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0503 17:41:25.407404  687371 out.go:177] üëç  Starting "multinode-demo" primary control-plane node in "multinode-demo" cluster
I0503 17:41:25.408624  687371 cache.go:121] Beginning downloading kic base image for docker with docker
I0503 17:41:25.409773  687371 out.go:177] üöú  Pulling base image v0.0.43 ...
I0503 17:41:25.411129  687371 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0503 17:41:25.411159  687371 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0503 17:41:25.411181  687371 preload.go:147] Found local preload: /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0503 17:41:25.411189  687371 cache.go:56] Caching tarball of preloaded images
I0503 17:41:25.411258  687371 preload.go:173] Found /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0503 17:41:25.411264  687371 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0503 17:41:25.411597  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:41:25.411614  687371 lock.go:35] WriteFile acquiring /home/princewillopah/.minikube/profiles/multinode-demo/config.json: {Name:mk4df5bda40ac69fb4c2df79a3ac9fa6c09bcdd2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:25.428495  687371 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0503 17:41:25.428512  687371 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0503 17:41:25.428537  687371 cache.go:194] Successfully downloaded all kic artifacts
I0503 17:41:25.428569  687371 start.go:360] acquireMachinesLock for multinode-demo: {Name:mkb41645565bdd6e1d929ba8fbc14c3a370f2139 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0503 17:41:25.428679  687371 start.go:364] duration metric: took 94.995¬µs to acquireMachinesLock for "multinode-demo"
I0503 17:41:25.428698  687371 start.go:93] Provisioning new machine with config: &{Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0503 17:41:25.428777  687371 start.go:125] createHost starting for "" (driver="docker")
I0503 17:41:25.431863  687371 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0503 17:41:25.432255  687371 start.go:159] libmachine.API.Create for "multinode-demo" (driver="docker")
I0503 17:41:25.432277  687371 client.go:168] LocalClient.Create starting
I0503 17:41:25.432348  687371 main.go:141] libmachine: Reading certificate data from /home/princewillopah/.minikube/certs/ca.pem
I0503 17:41:25.432396  687371 main.go:141] libmachine: Decoding PEM data...
I0503 17:41:25.432410  687371 main.go:141] libmachine: Parsing certificate...
I0503 17:41:25.432619  687371 main.go:141] libmachine: Reading certificate data from /home/princewillopah/.minikube/certs/cert.pem
I0503 17:41:25.432644  687371 main.go:141] libmachine: Decoding PEM data...
I0503 17:41:25.432652  687371 main.go:141] libmachine: Parsing certificate...
I0503 17:41:25.433116  687371 cli_runner.go:164] Run: docker network inspect multinode-demo --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0503 17:41:25.450073  687371 cli_runner.go:211] docker network inspect multinode-demo --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0503 17:41:25.450158  687371 network_create.go:281] running [docker network inspect multinode-demo] to gather additional debugging logs...
I0503 17:41:25.450172  687371 cli_runner.go:164] Run: docker network inspect multinode-demo
W0503 17:41:25.467622  687371 cli_runner.go:211] docker network inspect multinode-demo returned with exit code 1
I0503 17:41:25.467651  687371 network_create.go:284] error running [docker network inspect multinode-demo]: docker network inspect multinode-demo: exit status 1
stdout:
[]

stderr:
Error response from daemon: network multinode-demo not found
I0503 17:41:25.467665  687371 network_create.go:286] output of [docker network inspect multinode-demo]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network multinode-demo not found

** /stderr **
I0503 17:41:25.467796  687371 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0503 17:41:25.487126  687371 network.go:211] skipping subnet 192.168.49.0/24 that is taken: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName:br-3aa9c557ea1f IfaceIPv4:192.168.49.1 IfaceMTU:1500 IfaceMAC:02:42:e2:38:b2:96} reservation:<nil>}
I0503 17:41:25.487511  687371 network.go:211] skipping subnet 192.168.58.0/24 that is taken: &{IP:192.168.58.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.58.0/24 Gateway:192.168.58.1 ClientMin:192.168.58.2 ClientMax:192.168.58.254 Broadcast:192.168.58.255 IsPrivate:true Interface:{IfaceName:br-9ddc65c8440e IfaceIPv4:192.168.58.1 IfaceMTU:1500 IfaceMAC:02:42:f8:65:01:41} reservation:<nil>}
I0503 17:41:25.488031  687371 network.go:206] using free private subnet 192.168.67.0/24: &{IP:192.168.67.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.67.0/24 Gateway:192.168.67.1 ClientMin:192.168.67.2 ClientMax:192.168.67.254 Broadcast:192.168.67.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0021f0af0}
I0503 17:41:25.488054  687371 network_create.go:124] attempt to create docker network multinode-demo 192.168.67.0/24 with gateway 192.168.67.1 and MTU of 1500 ...
I0503 17:41:25.488119  687371 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.67.0/24 --gateway=192.168.67.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=multinode-demo multinode-demo
I0503 17:41:25.566590  687371 network_create.go:108] docker network multinode-demo 192.168.67.0/24 created
I0503 17:41:25.566610  687371 kic.go:121] calculated static IP "192.168.67.2" for the "multinode-demo" container
I0503 17:41:25.566675  687371 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0503 17:41:25.582787  687371 cli_runner.go:164] Run: docker volume create multinode-demo --label name.minikube.sigs.k8s.io=multinode-demo --label created_by.minikube.sigs.k8s.io=true
I0503 17:41:25.602208  687371 oci.go:103] Successfully created a docker volume multinode-demo
I0503 17:41:25.602287  687371 cli_runner.go:164] Run: docker run --rm --name multinode-demo-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=multinode-demo --entrypoint /usr/bin/test -v multinode-demo:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0503 17:41:26.215998  687371 oci.go:107] Successfully prepared a docker volume multinode-demo
I0503 17:41:26.216049  687371 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0503 17:41:26.216068  687371 kic.go:194] Starting extracting preloaded images to volume ...
I0503 17:41:26.216145  687371 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v multinode-demo:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0503 17:41:29.188424  687371 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v multinode-demo:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (2.972245341s)
I0503 17:41:29.188446  687371 kic.go:203] duration metric: took 2.972374446s to extract preloaded images to volume ...
W0503 17:41:29.188571  687371 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0503 17:41:29.188603  687371 oci.go:243] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0503 17:41:29.188648  687371 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0503 17:41:29.211889  687371 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname multinode-demo --name multinode-demo --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=multinode-demo --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=multinode-demo --network multinode-demo --ip 192.168.67.2 --volume multinode-demo:/var --security-opt apparmor=unconfined --memory=2200mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0503 17:41:29.564905  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Running}}
I0503 17:41:29.582679  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Status}}
I0503 17:41:29.606714  687371 cli_runner.go:164] Run: docker exec multinode-demo stat /var/lib/dpkg/alternatives/iptables
I0503 17:41:29.659886  687371 oci.go:144] the created container "multinode-demo" has a running status.
I0503 17:41:29.659907  687371 kic.go:225] Creating ssh key for kic: /home/princewillopah/.minikube/machines/multinode-demo/id_rsa...
I0503 17:41:30.102399  687371 kic_runner.go:191] docker (temp): /home/princewillopah/.minikube/machines/multinode-demo/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0503 17:41:30.133161  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Status}}
I0503 17:41:30.154784  687371 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0503 17:41:30.154795  687371 kic_runner.go:114] Args: [docker exec --privileged multinode-demo chown docker:docker /home/docker/.ssh/authorized_keys]
I0503 17:41:30.220350  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Status}}
I0503 17:41:30.263966  687371 machine.go:94] provisionDockerMachine start ...
I0503 17:41:30.264067  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:30.288650  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:30.288875  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32807 <nil> <nil>}
I0503 17:41:30.288882  687371 main.go:141] libmachine: About to run SSH command:
hostname
I0503 17:41:30.429965  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: multinode-demo

I0503 17:41:30.429997  687371 ubuntu.go:169] provisioning hostname "multinode-demo"
I0503 17:41:30.430084  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:30.457359  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:30.457529  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32807 <nil> <nil>}
I0503 17:41:30.457537  687371 main.go:141] libmachine: About to run SSH command:
sudo hostname multinode-demo && echo "multinode-demo" | sudo tee /etc/hostname
I0503 17:41:30.623325  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: multinode-demo

I0503 17:41:30.623393  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:30.643242  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:30.643412  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32807 <nil> <nil>}
I0503 17:41:30.643425  687371 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\smultinode-demo' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 multinode-demo/g' /etc/hosts;
			else 
				echo '127.0.1.1 multinode-demo' | sudo tee -a /etc/hosts; 
			fi
		fi
I0503 17:41:30.769898  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0503 17:41:30.769926  687371 ubuntu.go:175] set auth options {CertDir:/home/princewillopah/.minikube CaCertPath:/home/princewillopah/.minikube/certs/ca.pem CaPrivateKeyPath:/home/princewillopah/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/princewillopah/.minikube/machines/server.pem ServerKeyPath:/home/princewillopah/.minikube/machines/server-key.pem ClientKeyPath:/home/princewillopah/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/princewillopah/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/princewillopah/.minikube}
I0503 17:41:30.769953  687371 ubuntu.go:177] setting up certificates
I0503 17:41:30.769973  687371 provision.go:84] configureAuth start
I0503 17:41:30.770051  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo
I0503 17:41:30.787837  687371 provision.go:143] copyHostCerts
I0503 17:41:30.787899  687371 exec_runner.go:144] found /home/princewillopah/.minikube/ca.pem, removing ...
I0503 17:41:30.787906  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/ca.pem
I0503 17:41:30.788020  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/ca.pem --> /home/princewillopah/.minikube/ca.pem (1099 bytes)
I0503 17:41:30.788108  687371 exec_runner.go:144] found /home/princewillopah/.minikube/cert.pem, removing ...
I0503 17:41:30.788111  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/cert.pem
I0503 17:41:30.788143  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/cert.pem --> /home/princewillopah/.minikube/cert.pem (1143 bytes)
I0503 17:41:30.788201  687371 exec_runner.go:144] found /home/princewillopah/.minikube/key.pem, removing ...
I0503 17:41:30.788204  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/key.pem
I0503 17:41:30.788224  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/key.pem --> /home/princewillopah/.minikube/key.pem (1671 bytes)
I0503 17:41:30.788267  687371 provision.go:117] generating server cert: /home/princewillopah/.minikube/machines/server.pem ca-key=/home/princewillopah/.minikube/certs/ca.pem private-key=/home/princewillopah/.minikube/certs/ca-key.pem org=princewillopah.multinode-demo san=[127.0.0.1 192.168.67.2 localhost minikube multinode-demo]
I0503 17:41:30.917418  687371 provision.go:177] copyRemoteCerts
I0503 17:41:30.917480  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0503 17:41:30.917518  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:30.934757  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:41:31.022196  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0503 17:41:31.048751  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/machines/server.pem --> /etc/docker/server.pem (1233 bytes)
I0503 17:41:31.077457  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0503 17:41:31.106787  687371 provision.go:87] duration metric: took 336.801029ms to configureAuth
I0503 17:41:31.106810  687371 ubuntu.go:193] setting minikube options for container-runtime
I0503 17:41:31.106981  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:41:31.107028  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:31.125109  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:31.125284  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32807 <nil> <nil>}
I0503 17:41:31.125290  687371 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0503 17:41:31.245433  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0503 17:41:31.245446  687371 ubuntu.go:71] root file system type: overlay
I0503 17:41:31.245557  687371 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0503 17:41:31.245623  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:31.263602  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:31.263786  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32807 <nil> <nil>}
I0503 17:41:31.263846  687371 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0503 17:41:31.407139  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0503 17:41:31.407220  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:31.425823  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:31.426039  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32807 <nil> <nil>}
I0503 17:41:31.426052  687371 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0503 17:41:32.522431  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:59.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-03 17:41:31.400589167 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0503 17:41:32.522454  687371 machine.go:97] duration metric: took 2.258473384s to provisionDockerMachine
I0503 17:41:32.522463  687371 client.go:171] duration metric: took 7.090181732s to LocalClient.Create
I0503 17:41:32.522474  687371 start.go:167] duration metric: took 7.090221125s to libmachine.API.Create "multinode-demo"
I0503 17:41:32.522479  687371 start.go:293] postStartSetup for "multinode-demo" (driver="docker")
I0503 17:41:32.522488  687371 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0503 17:41:32.522543  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0503 17:41:32.522584  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:32.545249  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:41:32.635466  687371 ssh_runner.go:195] Run: cat /etc/os-release
I0503 17:41:32.639458  687371 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0503 17:41:32.639483  687371 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0503 17:41:32.639491  687371 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0503 17:41:32.639496  687371 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0503 17:41:32.639506  687371 filesync.go:126] Scanning /home/princewillopah/.minikube/addons for local assets ...
I0503 17:41:32.639579  687371 filesync.go:126] Scanning /home/princewillopah/.minikube/files for local assets ...
I0503 17:41:32.639594  687371 start.go:296] duration metric: took 117.110769ms for postStartSetup
I0503 17:41:32.640087  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo
I0503 17:41:32.657887  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:41:32.658137  687371 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0503 17:41:32.658171  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:32.675688  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:41:32.765562  687371 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0503 17:41:32.770266  687371 start.go:128] duration metric: took 7.34147647s to createHost
I0503 17:41:32.770282  687371 start.go:83] releasing machines lock for "multinode-demo", held for 7.341596838s
I0503 17:41:32.770350  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo
I0503 17:41:32.787405  687371 ssh_runner.go:195] Run: cat /version.json
I0503 17:41:32.787443  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:32.787460  687371 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0503 17:41:32.787508  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:32.809832  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:41:32.816542  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:41:32.984671  687371 ssh_runner.go:195] Run: systemctl --version
I0503 17:41:32.992867  687371 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0503 17:41:32.997371  687371 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0503 17:41:33.025385  687371 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0503 17:41:33.025447  687371 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0503 17:41:33.054134  687371 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0503 17:41:33.054158  687371 start.go:494] detecting cgroup driver to use...
I0503 17:41:33.054186  687371 detect.go:199] detected "systemd" cgroup driver on host os
I0503 17:41:33.054293  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0503 17:41:33.071320  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0503 17:41:33.082819  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0503 17:41:33.093938  687371 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0503 17:41:33.094000  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0503 17:41:33.104885  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0503 17:41:33.115484  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0503 17:41:33.126634  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0503 17:41:33.137216  687371 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0503 17:41:33.148026  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0503 17:41:33.158730  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0503 17:41:33.169747  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0503 17:41:33.181404  687371 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0503 17:41:33.190376  687371 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0503 17:41:33.198767  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:41:33.301235  687371 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0503 17:41:33.400343  687371 start.go:494] detecting cgroup driver to use...
I0503 17:41:33.400380  687371 detect.go:199] detected "systemd" cgroup driver on host os
I0503 17:41:33.400431  687371 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0503 17:41:33.414475  687371 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0503 17:41:33.414569  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0503 17:41:33.430852  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0503 17:41:33.453891  687371 ssh_runner.go:195] Run: which cri-dockerd
I0503 17:41:33.457737  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0503 17:41:33.468329  687371 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0503 17:41:33.489890  687371 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0503 17:41:33.617807  687371 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0503 17:41:33.737262  687371 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0503 17:41:33.737375  687371 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0503 17:41:33.757888  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:41:33.868788  687371 ssh_runner.go:195] Run: sudo systemctl restart docker
I0503 17:41:34.479280  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0503 17:41:34.493865  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0503 17:41:34.507816  687371 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0503 17:41:34.617001  687371 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0503 17:41:34.730450  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:41:34.850452  687371 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0503 17:41:34.865369  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0503 17:41:34.878677  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:41:34.997489  687371 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0503 17:41:35.074412  687371 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0503 17:41:35.074469  687371 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0503 17:41:35.079240  687371 start.go:562] Will wait 60s for crictl version
I0503 17:41:35.079296  687371 ssh_runner.go:195] Run: which crictl
I0503 17:41:35.084408  687371 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0503 17:41:35.127025  687371 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0503 17:41:35.127086  687371 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0503 17:41:35.151868  687371 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0503 17:41:35.187558  687371 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0503 17:41:35.187692  687371 cli_runner.go:164] Run: docker network inspect multinode-demo --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0503 17:41:35.207668  687371 ssh_runner.go:195] Run: grep 192.168.67.1	host.minikube.internal$ /etc/hosts
I0503 17:41:35.212209  687371 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.67.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0503 17:41:35.224728  687371 kubeadm.go:877] updating cluster {Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0503 17:41:35.224848  687371 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0503 17:41:35.224896  687371 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0503 17:41:35.246339  687371 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0503 17:41:35.246355  687371 docker.go:615] Images already preloaded, skipping extraction
I0503 17:41:35.246422  687371 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0503 17:41:35.265015  687371 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0503 17:41:35.265031  687371 cache_images.go:84] Images are preloaded, skipping loading
I0503 17:41:35.265046  687371 kubeadm.go:928] updating node { 192.168.67.2 8443 v1.30.0 docker true true} ...
I0503 17:41:35.265136  687371 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=multinode-demo --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.67.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0503 17:41:35.265193  687371 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0503 17:41:35.321111  687371 cni.go:84] Creating CNI manager for ""
I0503 17:41:35.321126  687371 cni.go:136] multinode detected (1 nodes found), recommending kindnet
I0503 17:41:35.321137  687371 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0503 17:41:35.321162  687371 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.67.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:multinode-demo NodeName:multinode-demo DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.67.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.67.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0503 17:41:35.321403  687371 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.67.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "multinode-demo"
  kubeletExtraArgs:
    node-ip: 192.168.67.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.67.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0503 17:41:35.321476  687371 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0503 17:41:35.331358  687371 binaries.go:44] Found k8s binaries, skipping transfer
I0503 17:41:35.331431  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0503 17:41:35.341019  687371 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (313 bytes)
I0503 17:41:35.362249  687371 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0503 17:41:35.384177  687371 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2155 bytes)
I0503 17:41:35.406922  687371 ssh_runner.go:195] Run: grep 192.168.67.2	control-plane.minikube.internal$ /etc/hosts
I0503 17:41:35.410796  687371 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.67.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0503 17:41:35.422583  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:41:35.537427  687371 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0503 17:41:35.560902  687371 certs.go:68] Setting up /home/princewillopah/.minikube/profiles/multinode-demo for IP: 192.168.67.2
I0503 17:41:35.560914  687371 certs.go:194] generating shared ca certs ...
I0503 17:41:35.560930  687371 certs.go:226] acquiring lock for ca certs: {Name:mk3ec114f777ab0e10c44ed8952ba18450ef5a1a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:35.561067  687371 certs.go:235] skipping valid "minikubeCA" ca cert: /home/princewillopah/.minikube/ca.key
I0503 17:41:35.561112  687371 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/princewillopah/.minikube/proxy-client-ca.key
I0503 17:41:35.561117  687371 certs.go:256] generating profile certs ...
I0503 17:41:35.561174  687371 certs.go:363] generating signed profile cert for "minikube-user": /home/princewillopah/.minikube/profiles/multinode-demo/client.key
I0503 17:41:35.561185  687371 crypto.go:68] Generating cert /home/princewillopah/.minikube/profiles/multinode-demo/client.crt with IP's: []
I0503 17:41:36.019103  687371 crypto.go:156] Writing cert to /home/princewillopah/.minikube/profiles/multinode-demo/client.crt ...
I0503 17:41:36.019124  687371 lock.go:35] WriteFile acquiring /home/princewillopah/.minikube/profiles/multinode-demo/client.crt: {Name:mk80fd232b2bc1f0cd46cc30794d85d61dc7b870 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:36.019341  687371 crypto.go:164] Writing key to /home/princewillopah/.minikube/profiles/multinode-demo/client.key ...
I0503 17:41:36.019348  687371 lock.go:35] WriteFile acquiring /home/princewillopah/.minikube/profiles/multinode-demo/client.key: {Name:mk9bb4a8b51a624efdc38557c7b47abee19bf298 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:36.019427  687371 certs.go:363] generating signed profile cert for "minikube": /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.key.f4eb9ab5
I0503 17:41:36.019437  687371 crypto.go:68] Generating cert /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.crt.f4eb9ab5 with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.67.2]
I0503 17:41:36.172159  687371 crypto.go:156] Writing cert to /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.crt.f4eb9ab5 ...
I0503 17:41:36.172176  687371 lock.go:35] WriteFile acquiring /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.crt.f4eb9ab5: {Name:mk025686443e2de6b3892f3a3fb3bbfc8a68dce6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:36.172354  687371 crypto.go:164] Writing key to /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.key.f4eb9ab5 ...
I0503 17:41:36.172360  687371 lock.go:35] WriteFile acquiring /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.key.f4eb9ab5: {Name:mkbd674cc24bd6249eef6a427aa31bebb57c9329 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:36.172426  687371 certs.go:381] copying /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.crt.f4eb9ab5 -> /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.crt
I0503 17:41:36.172524  687371 certs.go:385] copying /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.key.f4eb9ab5 -> /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.key
I0503 17:41:36.172577  687371 certs.go:363] generating signed profile cert for "aggregator": /home/princewillopah/.minikube/profiles/multinode-demo/proxy-client.key
I0503 17:41:36.172588  687371 crypto.go:68] Generating cert /home/princewillopah/.minikube/profiles/multinode-demo/proxy-client.crt with IP's: []
I0503 17:41:36.310306  687371 crypto.go:156] Writing cert to /home/princewillopah/.minikube/profiles/multinode-demo/proxy-client.crt ...
I0503 17:41:36.310322  687371 lock.go:35] WriteFile acquiring /home/princewillopah/.minikube/profiles/multinode-demo/proxy-client.crt: {Name:mk1940d608a396d948c48bc9cf563678d0867038 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:36.310526  687371 crypto.go:164] Writing key to /home/princewillopah/.minikube/profiles/multinode-demo/proxy-client.key ...
I0503 17:41:36.310533  687371 lock.go:35] WriteFile acquiring /home/princewillopah/.minikube/profiles/multinode-demo/proxy-client.key: {Name:mkd8b988337b540ed489454f84d212746efeeeb6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:36.310834  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/ca-key.pem (1675 bytes)
I0503 17:41:36.310873  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/ca.pem (1099 bytes)
I0503 17:41:36.310901  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/cert.pem (1143 bytes)
I0503 17:41:36.310924  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/key.pem (1671 bytes)
I0503 17:41:36.311648  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0503 17:41:36.341755  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0503 17:41:36.373028  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0503 17:41:36.406722  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0503 17:41:36.444895  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1419 bytes)
I0503 17:41:36.481126  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/profiles/multinode-demo/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0503 17:41:36.509341  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/profiles/multinode-demo/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0503 17:41:36.535994  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/profiles/multinode-demo/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0503 17:41:36.563763  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0503 17:41:36.591805  687371 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0503 17:41:36.613574  687371 ssh_runner.go:195] Run: openssl version
I0503 17:41:36.621327  687371 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0503 17:41:36.633302  687371 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0503 17:41:36.637189  687371 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  2 17:21 /usr/share/ca-certificates/minikubeCA.pem
I0503 17:41:36.637241  687371 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0503 17:41:36.644980  687371 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0503 17:41:36.656393  687371 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0503 17:41:36.660001  687371 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0503 17:41:36.660041  687371 kubeadm.go:391] StartCluster: {Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0503 17:41:36.660165  687371 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0503 17:41:36.678007  687371 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0503 17:41:36.689881  687371 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0503 17:41:36.700916  687371 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0503 17:41:36.700966  687371 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0503 17:41:36.713833  687371 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0503 17:41:36.713845  687371 kubeadm.go:156] found existing configuration files:

I0503 17:41:36.713907  687371 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0503 17:41:36.724834  687371 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0503 17:41:36.724902  687371 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0503 17:41:36.734555  687371 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0503 17:41:36.745473  687371 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0503 17:41:36.745522  687371 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0503 17:41:36.754926  687371 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0503 17:41:36.765654  687371 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0503 17:41:36.765713  687371 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0503 17:41:36.775245  687371 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0503 17:41:36.786155  687371 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0503 17:41:36.786215  687371 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0503 17:41:36.797745  687371 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0503 17:41:36.851785  687371 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0503 17:41:36.851847  687371 kubeadm.go:309] [preflight] Running pre-flight checks
I0503 17:41:36.894462  687371 kubeadm.go:309] [preflight] The system verification failed. Printing the output from the verification:
I0503 17:41:36.894521  687371 kubeadm.go:309] [0;37mKERNEL_VERSION[0m: [0;32m5.15.0-67-generic[0m
I0503 17:41:36.894550  687371 kubeadm.go:309] [0;37mOS[0m: [0;32mLinux[0m
I0503 17:41:36.894598  687371 kubeadm.go:309] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0503 17:41:36.894644  687371 kubeadm.go:309] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0503 17:41:36.894688  687371 kubeadm.go:309] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0503 17:41:36.894730  687371 kubeadm.go:309] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0503 17:41:36.894769  687371 kubeadm.go:309] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0503 17:41:36.894811  687371 kubeadm.go:309] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0503 17:41:36.894852  687371 kubeadm.go:309] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0503 17:41:36.894888  687371 kubeadm.go:309] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0503 17:41:36.976205  687371 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0503 17:41:36.976305  687371 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0503 17:41:36.976394  687371 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0503 17:41:37.268961  687371 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0503 17:41:37.272249  687371 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0503 17:41:37.272354  687371 kubeadm.go:309] [certs] Using existing ca certificate authority
I0503 17:41:37.272427  687371 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0503 17:41:37.414465  687371 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0503 17:41:37.546287  687371 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0503 17:41:37.826240  687371 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0503 17:41:38.043219  687371 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0503 17:41:38.157115  687371 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0503 17:41:38.157460  687371 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost multinode-demo] and IPs [192.168.67.2 127.0.0.1 ::1]
I0503 17:41:38.421721  687371 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0503 17:41:38.422007  687371 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost multinode-demo] and IPs [192.168.67.2 127.0.0.1 ::1]
I0503 17:41:38.624806  687371 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0503 17:41:38.940780  687371 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0503 17:41:39.059406  687371 kubeadm.go:309] [certs] Generating "sa" key and public key
I0503 17:41:39.059734  687371 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0503 17:41:39.200006  687371 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0503 17:41:39.285946  687371 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0503 17:41:39.685023  687371 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0503 17:41:39.937901  687371 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0503 17:41:40.128744  687371 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0503 17:41:40.129456  687371 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0503 17:41:40.134220  687371 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0503 17:41:40.135577  687371 out.go:204]     ‚ñ™ Booting up control plane ...
I0503 17:41:40.135692  687371 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0503 17:41:40.137245  687371 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0503 17:41:40.139183  687371 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0503 17:41:40.167369  687371 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0503 17:41:40.168624  687371 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0503 17:41:40.168665  687371 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0503 17:41:40.297562  687371 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0503 17:41:40.297635  687371 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0503 17:41:40.799046  687371 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 502.001924ms
I0503 17:41:40.799127  687371 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0503 17:41:45.801568  687371 kubeadm.go:309] [api-check] The API server is healthy after 5.002285703s
I0503 17:41:45.815492  687371 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0503 17:41:45.830488  687371 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0503 17:41:45.865778  687371 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0503 17:41:45.866101  687371 kubeadm.go:309] [mark-control-plane] Marking the node multinode-demo as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0503 17:41:45.880610  687371 kubeadm.go:309] [bootstrap-token] Using token: pekzg1.lww9cd2o48e1a89s
I0503 17:41:45.881789  687371 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I0503 17:41:45.881920  687371 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0503 17:41:45.887715  687371 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0503 17:41:45.898131  687371 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0503 17:41:45.903637  687371 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0503 17:41:45.908057  687371 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0503 17:41:45.913132  687371 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0503 17:41:46.210991  687371 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0503 17:41:46.635899  687371 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0503 17:41:47.209222  687371 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0503 17:41:47.210406  687371 kubeadm.go:309] 
I0503 17:41:47.210464  687371 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0503 17:41:47.210468  687371 kubeadm.go:309] 
I0503 17:41:47.210543  687371 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0503 17:41:47.210547  687371 kubeadm.go:309] 
I0503 17:41:47.210568  687371 kubeadm.go:309]   mkdir -p $HOME/.kube
I0503 17:41:47.210877  687371 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0503 17:41:47.210932  687371 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0503 17:41:47.210935  687371 kubeadm.go:309] 
I0503 17:41:47.210983  687371 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0503 17:41:47.210986  687371 kubeadm.go:309] 
I0503 17:41:47.211036  687371 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0503 17:41:47.211039  687371 kubeadm.go:309] 
I0503 17:41:47.211083  687371 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0503 17:41:47.211147  687371 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0503 17:41:47.211207  687371 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0503 17:41:47.211210  687371 kubeadm.go:309] 
I0503 17:41:47.211294  687371 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0503 17:41:47.211376  687371 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0503 17:41:47.211381  687371 kubeadm.go:309] 
I0503 17:41:47.211457  687371 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token pekzg1.lww9cd2o48e1a89s \
I0503 17:41:47.211555  687371 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:c3e1d0d5f571a14031e41fdd48f3c3c812a60614dd8862a7927597fb41b23776 \
I0503 17:41:47.211583  687371 kubeadm.go:309] 	--control-plane 
I0503 17:41:47.211587  687371 kubeadm.go:309] 
I0503 17:41:47.211683  687371 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0503 17:41:47.211686  687371 kubeadm.go:309] 
I0503 17:41:47.211763  687371 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token pekzg1.lww9cd2o48e1a89s \
I0503 17:41:47.211862  687371 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:c3e1d0d5f571a14031e41fdd48f3c3c812a60614dd8862a7927597fb41b23776 
I0503 17:41:47.216211  687371 kubeadm.go:309] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-67-generic\n", err: exit status 1
I0503 17:41:47.216307  687371 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0503 17:41:47.216321  687371 cni.go:84] Creating CNI manager for ""
I0503 17:41:47.216326  687371 cni.go:136] multinode detected (1 nodes found), recommending kindnet
I0503 17:41:47.217970  687371 out.go:177] üîó  Configuring CNI (Container Networking Interface) ...
I0503 17:41:47.218781  687371 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0503 17:41:47.224283  687371 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.30.0/kubectl ...
I0503 17:41:47.224295  687371 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0503 17:41:47.246228  687371 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0503 17:41:47.509333  687371 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0503 17:41:47.509446  687371 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes multinode-demo minikube.k8s.io/updated_at=2024_05_03T17_41_47_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=multinode-demo minikube.k8s.io/primary=true
I0503 17:41:47.509454  687371 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0503 17:41:47.636021  687371 kubeadm.go:1107] duration metric: took 126.549803ms to wait for elevateKubeSystemPrivileges
I0503 17:41:47.636057  687371 ops.go:34] apiserver oom_adj: -16
W0503 17:41:47.636085  687371 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0503 17:41:47.636090  687371 kubeadm.go:393] duration metric: took 10.976054161s to StartCluster
I0503 17:41:47.636105  687371 settings.go:142] acquiring lock: {Name:mk219324d32569f5b39bf4a618dbb198132cf7ca Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:47.636162  687371 settings.go:150] Updating kubeconfig:  /home/princewillopah/.kube/config
I0503 17:41:47.636845  687371 lock.go:35] WriteFile acquiring /home/princewillopah/.kube/config: {Name:mk85290ad4f1c20258171bdf458fc19e70499512 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:41:47.637044  687371 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0503 17:41:47.638433  687371 out.go:177] üîé  Verifying Kubernetes components...
I0503 17:41:47.637120  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0503 17:41:47.637252  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:41:47.637259  687371 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0503 17:41:47.639395  687371 addons.go:69] Setting storage-provisioner=true in profile "multinode-demo"
I0503 17:41:47.639420  687371 addons.go:234] Setting addon storage-provisioner=true in "multinode-demo"
I0503 17:41:47.639436  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:41:47.639447  687371 host.go:66] Checking if "multinode-demo" exists ...
I0503 17:41:47.639579  687371 addons.go:69] Setting default-storageclass=true in profile "multinode-demo"
I0503 17:41:47.639598  687371 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "multinode-demo"
I0503 17:41:47.639795  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Status}}
I0503 17:41:47.639844  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Status}}
I0503 17:41:47.673885  687371 addons.go:234] Setting addon default-storageclass=true in "multinode-demo"
I0503 17:41:47.673914  687371 host.go:66] Checking if "multinode-demo" exists ...
I0503 17:41:47.674212  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Status}}
I0503 17:41:47.684775  687371 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0503 17:41:47.685726  687371 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0503 17:41:47.685736  687371 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0503 17:41:47.685795  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:47.720749  687371 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0503 17:41:47.720762  687371 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0503 17:41:47.720815  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:41:47.747845  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:41:47.753030  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:41:47.956929  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.67.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0503 17:41:47.957011  687371 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0503 17:41:47.964886  687371 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0503 17:41:48.028229  687371 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0503 17:41:48.354127  687371 api_server.go:52] waiting for apiserver process to appear ...
I0503 17:41:48.354177  687371 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0503 17:41:48.354257  687371 start.go:946] {"host.minikube.internal": 192.168.67.1} host record injected into CoreDNS's ConfigMap
I0503 17:41:48.549572  687371 api_server.go:72] duration metric: took 912.507454ms to wait for apiserver process to appear ...
I0503 17:41:48.549582  687371 api_server.go:88] waiting for apiserver healthz status ...
I0503 17:41:48.549595  687371 api_server.go:253] Checking apiserver healthz at https://192.168.67.2:8443/healthz ...
I0503 17:41:48.565413  687371 api_server.go:279] https://192.168.67.2:8443/healthz returned 200:
ok
I0503 17:41:48.566478  687371 api_server.go:141] control plane version: v1.30.0
I0503 17:41:48.566493  687371 api_server.go:131] duration metric: took 16.905722ms to wait for apiserver health ...
I0503 17:41:48.566509  687371 system_pods.go:43] waiting for kube-system pods to appear ...
I0503 17:41:48.581245  687371 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0503 17:41:48.582757  687371 addons.go:505] duration metric: took 945.487861ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0503 17:41:48.587613  687371 system_pods.go:59] 5 kube-system pods found
I0503 17:41:48.587634  687371 system_pods.go:61] "etcd-multinode-demo" [a693fee7-55b3-4300-8dd1-1da91a782d7b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0503 17:41:48.587639  687371 system_pods.go:61] "kube-apiserver-multinode-demo" [3b4138d7-bf1a-429a-8256-56f345988d3c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0503 17:41:48.587645  687371 system_pods.go:61] "kube-controller-manager-multinode-demo" [03a23284-7c77-4989-9d22-98ab65bd4785] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0503 17:41:48.587650  687371 system_pods.go:61] "kube-scheduler-multinode-demo" [4047b7e3-6e58-4f8e-8cab-928fd99655d1] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0503 17:41:48.587655  687371 system_pods.go:61] "storage-provisioner" [32b2781c-2661-4444-b769-48bc901ac95a] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0503 17:41:48.587662  687371 system_pods.go:74] duration metric: took 21.148066ms to wait for pod list to return data ...
I0503 17:41:48.587682  687371 kubeadm.go:576] duration metric: took 950.619358ms to wait for: map[apiserver:true system_pods:true]
I0503 17:41:48.587692  687371 node_conditions.go:102] verifying NodePressure condition ...
I0503 17:41:48.599100  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:41:48.599135  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:41:48.599147  687371 node_conditions.go:105] duration metric: took 11.449363ms to run NodePressure ...
I0503 17:41:48.599161  687371 start.go:240] waiting for startup goroutines ...
I0503 17:41:48.862845  687371 kapi.go:248] "coredns" deployment in "kube-system" namespace and "multinode-demo" context rescaled to 1 replicas
I0503 17:41:48.862880  687371 start.go:245] waiting for cluster config update ...
I0503 17:41:48.862907  687371 start.go:254] writing updated cluster config ...
I0503 17:41:48.872917  687371 out.go:177] 
I0503 17:41:48.886775  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:41:48.886851  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:41:48.890098  687371 out.go:177] üëç  Starting "multinode-demo-m02" worker node in "multinode-demo" cluster
I0503 17:41:48.910470  687371 cache.go:121] Beginning downloading kic base image for docker with docker
I0503 17:41:48.911566  687371 out.go:177] üöú  Pulling base image v0.0.43 ...
I0503 17:41:48.913026  687371 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0503 17:41:48.913050  687371 cache.go:56] Caching tarball of preloaded images
I0503 17:41:48.913100  687371 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0503 17:41:48.913172  687371 preload.go:173] Found /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0503 17:41:48.913180  687371 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0503 17:41:48.913264  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:41:48.930145  687371 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0503 17:41:48.930165  687371 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0503 17:41:48.930189  687371 cache.go:194] Successfully downloaded all kic artifacts
I0503 17:41:48.930230  687371 start.go:360] acquireMachinesLock for multinode-demo-m02: {Name:mkfc5eadc59ee78f4fa3d555763360d60f6a958f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0503 17:41:48.930396  687371 start.go:364] duration metric: took 145.287¬µs to acquireMachinesLock for "multinode-demo-m02"
I0503 17:41:48.930424  687371 start.go:93] Provisioning new machine with config: &{Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name:m02 IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0503 17:41:48.930514  687371 start.go:125] createHost starting for "m02" (driver="docker")
I0503 17:41:48.933914  687371 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0503 17:41:48.934098  687371 start.go:159] libmachine.API.Create for "multinode-demo" (driver="docker")
I0503 17:41:48.934112  687371 client.go:168] LocalClient.Create starting
I0503 17:41:48.934172  687371 main.go:141] libmachine: Reading certificate data from /home/princewillopah/.minikube/certs/ca.pem
I0503 17:41:48.934204  687371 main.go:141] libmachine: Decoding PEM data...
I0503 17:41:48.934215  687371 main.go:141] libmachine: Parsing certificate...
I0503 17:41:48.934268  687371 main.go:141] libmachine: Reading certificate data from /home/princewillopah/.minikube/certs/cert.pem
I0503 17:41:48.934280  687371 main.go:141] libmachine: Decoding PEM data...
I0503 17:41:48.934287  687371 main.go:141] libmachine: Parsing certificate...
I0503 17:41:48.934491  687371 cli_runner.go:164] Run: docker network inspect multinode-demo --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0503 17:41:48.951541  687371 network_create.go:77] Found existing network {name:multinode-demo subnet:0xc00318a3f0 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 67 1] mtu:1500}
I0503 17:41:48.951580  687371 kic.go:121] calculated static IP "192.168.67.3" for the "multinode-demo-m02" container
I0503 17:41:48.951642  687371 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0503 17:41:48.968937  687371 cli_runner.go:164] Run: docker volume create multinode-demo-m02 --label name.minikube.sigs.k8s.io=multinode-demo-m02 --label created_by.minikube.sigs.k8s.io=true
I0503 17:41:48.987224  687371 oci.go:103] Successfully created a docker volume multinode-demo-m02
I0503 17:41:48.987293  687371 cli_runner.go:164] Run: docker run --rm --name multinode-demo-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=multinode-demo-m02 --entrypoint /usr/bin/test -v multinode-demo-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0503 17:41:49.562597  687371 oci.go:107] Successfully prepared a docker volume multinode-demo-m02
I0503 17:41:49.562629  687371 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0503 17:41:49.562651  687371 kic.go:194] Starting extracting preloaded images to volume ...
I0503 17:41:49.562709  687371 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v multinode-demo-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0503 17:41:52.735697  687371 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v multinode-demo-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (3.172953816s)
I0503 17:41:52.735718  687371 kic.go:203] duration metric: took 3.173064963s to extract preloaded images to volume ...
W0503 17:41:52.735788  687371 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0503 17:41:52.735815  687371 oci.go:243] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0503 17:41:52.735858  687371 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0503 17:41:52.766039  687371 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname multinode-demo-m02 --name multinode-demo-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=multinode-demo-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=multinode-demo-m02 --network multinode-demo --ip 192.168.67.3 --volume multinode-demo-m02:/var --security-opt apparmor=unconfined --memory=2200mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0503 17:41:53.125413  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m02 --format={{.State.Running}}
I0503 17:41:53.144656  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m02 --format={{.State.Status}}
I0503 17:41:53.168929  687371 cli_runner.go:164] Run: docker exec multinode-demo-m02 stat /var/lib/dpkg/alternatives/iptables
I0503 17:41:53.245256  687371 oci.go:144] the created container "multinode-demo-m02" has a running status.
I0503 17:41:53.245283  687371 kic.go:225] Creating ssh key for kic: /home/princewillopah/.minikube/machines/multinode-demo-m02/id_rsa...
I0503 17:41:53.406323  687371 kic_runner.go:191] docker (temp): /home/princewillopah/.minikube/machines/multinode-demo-m02/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0503 17:41:53.433225  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m02 --format={{.State.Status}}
I0503 17:41:53.476646  687371 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0503 17:41:53.476657  687371 kic_runner.go:114] Args: [docker exec --privileged multinode-demo-m02 chown docker:docker /home/docker/.ssh/authorized_keys]
I0503 17:41:53.534786  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m02 --format={{.State.Status}}
I0503 17:41:53.566070  687371 machine.go:94] provisionDockerMachine start ...
I0503 17:41:53.566157  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:53.602220  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:53.602396  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32812 <nil> <nil>}
I0503 17:41:53.602402  687371 main.go:141] libmachine: About to run SSH command:
hostname
I0503 17:41:53.603805  687371 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:46456->127.0.0.1:32812: read: connection reset by peer
I0503 17:41:56.724508  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: multinode-demo-m02

I0503 17:41:56.724524  687371 ubuntu.go:169] provisioning hostname "multinode-demo-m02"
I0503 17:41:56.724586  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:56.741517  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:56.741683  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32812 <nil> <nil>}
I0503 17:41:56.741690  687371 main.go:141] libmachine: About to run SSH command:
sudo hostname multinode-demo-m02 && echo "multinode-demo-m02" | sudo tee /etc/hostname
I0503 17:41:56.896334  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: multinode-demo-m02

I0503 17:41:56.896397  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:56.915272  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:56.915439  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32812 <nil> <nil>}
I0503 17:41:56.915451  687371 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\smultinode-demo-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 multinode-demo-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 multinode-demo-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I0503 17:41:57.037553  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0503 17:41:57.037576  687371 ubuntu.go:175] set auth options {CertDir:/home/princewillopah/.minikube CaCertPath:/home/princewillopah/.minikube/certs/ca.pem CaPrivateKeyPath:/home/princewillopah/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/princewillopah/.minikube/machines/server.pem ServerKeyPath:/home/princewillopah/.minikube/machines/server-key.pem ClientKeyPath:/home/princewillopah/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/princewillopah/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/princewillopah/.minikube}
I0503 17:41:57.037615  687371 ubuntu.go:177] setting up certificates
I0503 17:41:57.037627  687371 provision.go:84] configureAuth start
I0503 17:41:57.037688  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo-m02
I0503 17:41:57.055575  687371 provision.go:143] copyHostCerts
I0503 17:41:57.055625  687371 exec_runner.go:144] found /home/princewillopah/.minikube/cert.pem, removing ...
I0503 17:41:57.055632  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/cert.pem
I0503 17:41:57.055702  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/cert.pem --> /home/princewillopah/.minikube/cert.pem (1143 bytes)
I0503 17:41:57.055786  687371 exec_runner.go:144] found /home/princewillopah/.minikube/key.pem, removing ...
I0503 17:41:57.055790  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/key.pem
I0503 17:41:57.055812  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/key.pem --> /home/princewillopah/.minikube/key.pem (1671 bytes)
I0503 17:41:57.055857  687371 exec_runner.go:144] found /home/princewillopah/.minikube/ca.pem, removing ...
I0503 17:41:57.055860  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/ca.pem
I0503 17:41:57.055877  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/ca.pem --> /home/princewillopah/.minikube/ca.pem (1099 bytes)
I0503 17:41:57.055916  687371 provision.go:117] generating server cert: /home/princewillopah/.minikube/machines/server.pem ca-key=/home/princewillopah/.minikube/certs/ca.pem private-key=/home/princewillopah/.minikube/certs/ca-key.pem org=princewillopah.multinode-demo-m02 san=[127.0.0.1 192.168.67.3 localhost minikube multinode-demo-m02]
I0503 17:41:57.283767  687371 provision.go:177] copyRemoteCerts
I0503 17:41:57.283833  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0503 17:41:57.283880  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:57.301455  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32812 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m02/id_rsa Username:docker}
I0503 17:41:57.393316  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0503 17:41:57.423342  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/machines/server.pem --> /etc/docker/server.pem (1241 bytes)
I0503 17:41:57.454332  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0503 17:41:57.483838  687371 provision.go:87] duration metric: took 446.196932ms to configureAuth
I0503 17:41:57.483858  687371 ubuntu.go:193] setting minikube options for container-runtime
I0503 17:41:57.484157  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:41:57.484233  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:57.502557  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:57.502738  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32812 <nil> <nil>}
I0503 17:41:57.502745  687371 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0503 17:41:57.627511  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0503 17:41:57.627524  687371 ubuntu.go:71] root file system type: overlay
I0503 17:41:57.627643  687371 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0503 17:41:57.627705  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:57.646075  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:57.646312  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32812 <nil> <nil>}
I0503 17:41:57.646437  687371 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.67.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0503 17:41:57.782472  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.67.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0503 17:41:57.782600  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:57.802808  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:41:57.803033  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32812 <nil> <nil>}
I0503 17:41:57.803046  687371 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0503 17:41:59.011309  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:59.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-03 17:41:57.772881656 +0000
@@ -1,46 +1,50 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Environment=NO_PROXY=192.168.67.2
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0503 17:41:59.011330  687371 machine.go:97] duration metric: took 5.445240574s to provisionDockerMachine
I0503 17:41:59.011340  687371 client.go:171] duration metric: took 10.077223485s to LocalClient.Create
I0503 17:41:59.011353  687371 start.go:167] duration metric: took 10.077256501s to libmachine.API.Create "multinode-demo"
I0503 17:41:59.011359  687371 start.go:293] postStartSetup for "multinode-demo-m02" (driver="docker")
I0503 17:41:59.011367  687371 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0503 17:41:59.011424  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0503 17:41:59.011457  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:59.044159  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32812 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m02/id_rsa Username:docker}
I0503 17:41:59.150573  687371 ssh_runner.go:195] Run: cat /etc/os-release
I0503 17:41:59.154708  687371 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0503 17:41:59.154729  687371 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0503 17:41:59.154736  687371 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0503 17:41:59.154742  687371 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0503 17:41:59.154752  687371 filesync.go:126] Scanning /home/princewillopah/.minikube/addons for local assets ...
I0503 17:41:59.154800  687371 filesync.go:126] Scanning /home/princewillopah/.minikube/files for local assets ...
I0503 17:41:59.154814  687371 start.go:296] duration metric: took 143.45154ms for postStartSetup
I0503 17:41:59.155156  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo-m02
I0503 17:41:59.173267  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:41:59.173545  687371 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0503 17:41:59.173590  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:59.205130  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32812 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m02/id_rsa Username:docker}
I0503 17:41:59.301333  687371 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0503 17:41:59.309926  687371 start.go:128] duration metric: took 10.379387897s to createHost
I0503 17:41:59.309948  687371 start.go:83] releasing machines lock for "multinode-demo-m02", held for 10.379542038s
I0503 17:41:59.310037  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo-m02
I0503 17:41:59.335446  687371 out.go:177] üåê  Found network options:
I0503 17:41:59.336431  687371 out.go:177]     ‚ñ™ NO_PROXY=192.168.67.2
W0503 17:41:59.337274  687371 proxy.go:119] fail to check proxy env: Error ip not in block
W0503 17:41:59.337313  687371 proxy.go:119] fail to check proxy env: Error ip not in block
I0503 17:41:59.337413  687371 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0503 17:41:59.337450  687371 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0503 17:41:59.337458  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:59.337501  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m02
I0503 17:41:59.373437  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32812 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m02/id_rsa Username:docker}
I0503 17:41:59.383046  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32812 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m02/id_rsa Username:docker}
I0503 17:41:59.478062  687371 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0503 17:41:59.589943  687371 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0503 17:41:59.590011  687371 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0503 17:41:59.624348  687371 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0503 17:41:59.624365  687371 start.go:494] detecting cgroup driver to use...
I0503 17:41:59.624393  687371 detect.go:199] detected "systemd" cgroup driver on host os
I0503 17:41:59.624546  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0503 17:41:59.645146  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0503 17:41:59.658177  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0503 17:41:59.674533  687371 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0503 17:41:59.674590  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0503 17:41:59.690877  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0503 17:41:59.703334  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0503 17:41:59.715913  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0503 17:41:59.730482  687371 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0503 17:41:59.744678  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0503 17:41:59.758572  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0503 17:41:59.772291  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0503 17:41:59.784511  687371 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0503 17:41:59.794964  687371 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0503 17:41:59.804556  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:41:59.937536  687371 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0503 17:42:00.067201  687371 start.go:494] detecting cgroup driver to use...
I0503 17:42:00.067236  687371 detect.go:199] detected "systemd" cgroup driver on host os
I0503 17:42:00.067289  687371 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0503 17:42:00.098529  687371 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0503 17:42:00.098599  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0503 17:42:00.133778  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0503 17:42:00.160411  687371 ssh_runner.go:195] Run: which cri-dockerd
I0503 17:42:00.165032  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0503 17:42:00.181468  687371 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0503 17:42:00.215431  687371 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0503 17:42:00.433244  687371 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0503 17:42:00.672217  687371 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0503 17:42:00.672257  687371 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0503 17:42:00.718399  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:00.937273  687371 ssh_runner.go:195] Run: sudo systemctl restart docker
I0503 17:42:01.595079  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0503 17:42:01.613262  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0503 17:42:01.632048  687371 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0503 17:42:01.777807  687371 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0503 17:42:01.925676  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:02.070586  687371 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0503 17:42:02.087443  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0503 17:42:02.109911  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:02.250569  687371 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0503 17:42:02.375802  687371 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0503 17:42:02.375870  687371 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0503 17:42:02.380676  687371 start.go:562] Will wait 60s for crictl version
I0503 17:42:02.380748  687371 ssh_runner.go:195] Run: which crictl
I0503 17:42:02.385468  687371 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0503 17:42:02.438193  687371 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0503 17:42:02.438282  687371 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0503 17:42:02.478306  687371 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0503 17:42:02.508547  687371 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0503 17:42:02.509298  687371 out.go:177]     ‚ñ™ env NO_PROXY=192.168.67.2
I0503 17:42:02.510072  687371 cli_runner.go:164] Run: docker network inspect multinode-demo --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0503 17:42:02.527876  687371 ssh_runner.go:195] Run: grep 192.168.67.1	host.minikube.internal$ /etc/hosts
I0503 17:42:02.532537  687371 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.67.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0503 17:42:02.545110  687371 mustload.go:65] Loading cluster: multinode-demo
I0503 17:42:02.545304  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:02.545500  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Status}}
I0503 17:42:02.564333  687371 host.go:66] Checking if "multinode-demo" exists ...
I0503 17:42:02.564726  687371 certs.go:68] Setting up /home/princewillopah/.minikube/profiles/multinode-demo for IP: 192.168.67.3
I0503 17:42:02.564732  687371 certs.go:194] generating shared ca certs ...
I0503 17:42:02.564748  687371 certs.go:226] acquiring lock for ca certs: {Name:mk3ec114f777ab0e10c44ed8952ba18450ef5a1a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:42:02.564896  687371 certs.go:235] skipping valid "minikubeCA" ca cert: /home/princewillopah/.minikube/ca.key
I0503 17:42:02.564943  687371 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/princewillopah/.minikube/proxy-client-ca.key
I0503 17:42:02.565022  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/ca-key.pem (1675 bytes)
I0503 17:42:02.565046  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/ca.pem (1099 bytes)
I0503 17:42:02.565062  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/cert.pem (1143 bytes)
I0503 17:42:02.565076  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/key.pem (1671 bytes)
I0503 17:42:02.565123  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0503 17:42:02.596351  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0503 17:42:02.628833  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0503 17:42:02.663901  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0503 17:42:02.694150  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0503 17:42:02.722645  687371 ssh_runner.go:195] Run: openssl version
I0503 17:42:02.728457  687371 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0503 17:42:02.739425  687371 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0503 17:42:02.743593  687371 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  2 17:21 /usr/share/ca-certificates/minikubeCA.pem
I0503 17:42:02.743649  687371 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0503 17:42:02.751439  687371 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0503 17:42:02.761810  687371 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0503 17:42:02.767373  687371 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0503 17:42:02.767405  687371 kubeadm.go:928] updating node {m02 192.168.67.3 8443 v1.30.0 docker false true} ...
I0503 17:42:02.767493  687371 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=multinode-demo-m02 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.67.3

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0503 17:42:02.767557  687371 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0503 17:42:02.781500  687371 binaries.go:44] Found k8s binaries, skipping transfer
I0503 17:42:02.781555  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0503 17:42:02.791034  687371 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (317 bytes)
I0503 17:42:02.812640  687371 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0503 17:42:02.836778  687371 ssh_runner.go:195] Run: grep 192.168.67.2	control-plane.minikube.internal$ /etc/hosts
I0503 17:42:02.841014  687371 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.67.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0503 17:42:02.853456  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:02.970128  687371 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0503 17:42:02.995496  687371 host.go:66] Checking if "multinode-demo" exists ...
I0503 17:42:02.995722  687371 start.go:316] joinCluster: &{Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.67.3 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0503 17:42:02.995803  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I0503 17:42:02.995862  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:42:03.024065  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:42:03.241727  687371 start.go:342] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.67.3 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0503 17:42:03.241784  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token ympu2v.mgrjzfrdtdzaqy0v --discovery-token-ca-cert-hash sha256:c3e1d0d5f571a14031e41fdd48f3c3c812a60614dd8862a7927597fb41b23776 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=multinode-demo-m02"
I0503 17:42:04.772525  687371 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token ympu2v.mgrjzfrdtdzaqy0v --discovery-token-ca-cert-hash sha256:c3e1d0d5f571a14031e41fdd48f3c3c812a60614dd8862a7927597fb41b23776 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=multinode-demo-m02": (1.530717895s)
I0503 17:42:04.772545  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0503 17:42:05.107056  687371 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes multinode-demo-m02 minikube.k8s.io/updated_at=2024_05_03T17_42_05_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=multinode-demo minikube.k8s.io/primary=false
I0503 17:42:05.283867  687371 start.go:318] duration metric: took 2.28814009s to joinCluster
I0503 17:42:05.284011  687371 start.go:234] Will wait 6m0s for node &{Name:m02 IP:192.168.67.3 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0503 17:42:05.290920  687371 out.go:177] üîé  Verifying Kubernetes components...
I0503 17:42:05.284144  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:05.327831  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:05.463493  687371 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0503 17:42:05.478552  687371 kubeadm.go:576] duration metric: took 194.503967ms to wait for: map[apiserver:true system_pods:true]
I0503 17:42:05.478572  687371 node_conditions.go:102] verifying NodePressure condition ...
I0503 17:42:05.482075  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:42:05.482089  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:42:05.482098  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:42:05.482101  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:42:05.482104  687371 node_conditions.go:105] duration metric: took 3.528708ms to run NodePressure ...
I0503 17:42:05.482115  687371 start.go:240] waiting for startup goroutines ...
I0503 17:42:05.482136  687371 start.go:254] writing updated cluster config ...
I0503 17:42:05.483568  687371 out.go:177] 
I0503 17:42:05.484661  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:05.484748  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:42:05.485858  687371 out.go:177] üëç  Starting "multinode-demo-m03" worker node in "multinode-demo" cluster
I0503 17:42:05.486517  687371 cache.go:121] Beginning downloading kic base image for docker with docker
I0503 17:42:05.487065  687371 out.go:177] üöú  Pulling base image v0.0.43 ...
I0503 17:42:05.487852  687371 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0503 17:42:05.487866  687371 cache.go:56] Caching tarball of preloaded images
I0503 17:42:05.487881  687371 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0503 17:42:05.488115  687371 preload.go:173] Found /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0503 17:42:05.488122  687371 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0503 17:42:05.488269  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:42:05.505518  687371 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0503 17:42:05.505534  687371 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0503 17:42:05.505551  687371 cache.go:194] Successfully downloaded all kic artifacts
I0503 17:42:05.505584  687371 start.go:360] acquireMachinesLock for multinode-demo-m03: {Name:mk16199e3aaa4967bf2df20fa9f04bf02431c2f2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0503 17:42:05.505749  687371 start.go:364] duration metric: took 90.825¬µs to acquireMachinesLock for "multinode-demo-m03"
I0503 17:42:05.505779  687371 start.go:93] Provisioning new machine with config: &{Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.67.3 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name:m03 IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0503 17:42:05.505871  687371 start.go:125] createHost starting for "m03" (driver="docker")
I0503 17:42:05.508115  687371 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0503 17:42:05.508249  687371 start.go:159] libmachine.API.Create for "multinode-demo" (driver="docker")
I0503 17:42:05.508260  687371 client.go:168] LocalClient.Create starting
I0503 17:42:05.508306  687371 main.go:141] libmachine: Reading certificate data from /home/princewillopah/.minikube/certs/ca.pem
I0503 17:42:05.508335  687371 main.go:141] libmachine: Decoding PEM data...
I0503 17:42:05.508346  687371 main.go:141] libmachine: Parsing certificate...
I0503 17:42:05.508395  687371 main.go:141] libmachine: Reading certificate data from /home/princewillopah/.minikube/certs/cert.pem
I0503 17:42:05.508408  687371 main.go:141] libmachine: Decoding PEM data...
I0503 17:42:05.508415  687371 main.go:141] libmachine: Parsing certificate...
I0503 17:42:05.508652  687371 cli_runner.go:164] Run: docker network inspect multinode-demo --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0503 17:42:05.526695  687371 network_create.go:77] Found existing network {name:multinode-demo subnet:0xc00380cbd0 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 67 1] mtu:1500}
I0503 17:42:05.526725  687371 kic.go:121] calculated static IP "192.168.67.4" for the "multinode-demo-m03" container
I0503 17:42:05.526808  687371 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0503 17:42:05.547649  687371 cli_runner.go:164] Run: docker volume create multinode-demo-m03 --label name.minikube.sigs.k8s.io=multinode-demo-m03 --label created_by.minikube.sigs.k8s.io=true
I0503 17:42:05.563678  687371 oci.go:103] Successfully created a docker volume multinode-demo-m03
I0503 17:42:05.563773  687371 cli_runner.go:164] Run: docker run --rm --name multinode-demo-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=multinode-demo-m03 --entrypoint /usr/bin/test -v multinode-demo-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0503 17:42:06.147034  687371 oci.go:107] Successfully prepared a docker volume multinode-demo-m03
I0503 17:42:06.147088  687371 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0503 17:42:06.147110  687371 kic.go:194] Starting extracting preloaded images to volume ...
I0503 17:42:06.147179  687371 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v multinode-demo-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0503 17:42:10.684953  687371 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v multinode-demo-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (4.537741337s)
I0503 17:42:10.684981  687371 kic.go:203] duration metric: took 4.53786807s to extract preloaded images to volume ...
W0503 17:42:10.685060  687371 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0503 17:42:10.685084  687371 oci.go:243] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0503 17:42:10.685132  687371 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0503 17:42:10.731217  687371 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname multinode-demo-m03 --name multinode-demo-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=multinode-demo-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=multinode-demo-m03 --network multinode-demo --ip 192.168.67.4 --volume multinode-demo-m03:/var --security-opt apparmor=unconfined --memory=2200mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0503 17:42:11.366550  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m03 --format={{.State.Running}}
I0503 17:42:11.390869  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m03 --format={{.State.Status}}
I0503 17:42:11.415083  687371 cli_runner.go:164] Run: docker exec multinode-demo-m03 stat /var/lib/dpkg/alternatives/iptables
I0503 17:42:11.510529  687371 oci.go:144] the created container "multinode-demo-m03" has a running status.
I0503 17:42:11.510555  687371 kic.go:225] Creating ssh key for kic: /home/princewillopah/.minikube/machines/multinode-demo-m03/id_rsa...
I0503 17:42:11.890965  687371 kic_runner.go:191] docker (temp): /home/princewillopah/.minikube/machines/multinode-demo-m03/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0503 17:42:11.921778  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m03 --format={{.State.Status}}
I0503 17:42:11.957654  687371 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0503 17:42:11.957666  687371 kic_runner.go:114] Args: [docker exec --privileged multinode-demo-m03 chown docker:docker /home/docker/.ssh/authorized_keys]
I0503 17:42:12.057806  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m03 --format={{.State.Status}}
I0503 17:42:12.085507  687371 machine.go:94] provisionDockerMachine start ...
I0503 17:42:12.085591  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:12.122698  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:12.122886  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32817 <nil> <nil>}
I0503 17:42:12.122892  687371 main.go:141] libmachine: About to run SSH command:
hostname
I0503 17:42:12.124034  687371 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:35118->127.0.0.1:32817: read: connection reset by peer
I0503 17:42:15.265354  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: multinode-demo-m03

I0503 17:42:15.265369  687371 ubuntu.go:169] provisioning hostname "multinode-demo-m03"
I0503 17:42:15.265426  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:15.296690  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:15.296861  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32817 <nil> <nil>}
I0503 17:42:15.296871  687371 main.go:141] libmachine: About to run SSH command:
sudo hostname multinode-demo-m03 && echo "multinode-demo-m03" | sudo tee /etc/hostname
I0503 17:42:15.447820  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: multinode-demo-m03

I0503 17:42:15.447901  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:15.481296  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:15.481607  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32817 <nil> <nil>}
I0503 17:42:15.481621  687371 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\smultinode-demo-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 multinode-demo-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 multinode-demo-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I0503 17:42:15.630014  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0503 17:42:15.630033  687371 ubuntu.go:175] set auth options {CertDir:/home/princewillopah/.minikube CaCertPath:/home/princewillopah/.minikube/certs/ca.pem CaPrivateKeyPath:/home/princewillopah/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/princewillopah/.minikube/machines/server.pem ServerKeyPath:/home/princewillopah/.minikube/machines/server-key.pem ClientKeyPath:/home/princewillopah/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/princewillopah/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/princewillopah/.minikube}
I0503 17:42:15.630049  687371 ubuntu.go:177] setting up certificates
I0503 17:42:15.630059  687371 provision.go:84] configureAuth start
I0503 17:42:15.630139  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo-m03
I0503 17:42:15.651174  687371 provision.go:143] copyHostCerts
I0503 17:42:15.651229  687371 exec_runner.go:144] found /home/princewillopah/.minikube/ca.pem, removing ...
I0503 17:42:15.651236  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/ca.pem
I0503 17:42:15.651306  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/ca.pem --> /home/princewillopah/.minikube/ca.pem (1099 bytes)
I0503 17:42:15.651390  687371 exec_runner.go:144] found /home/princewillopah/.minikube/cert.pem, removing ...
I0503 17:42:15.651393  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/cert.pem
I0503 17:42:15.651423  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/cert.pem --> /home/princewillopah/.minikube/cert.pem (1143 bytes)
I0503 17:42:15.651469  687371 exec_runner.go:144] found /home/princewillopah/.minikube/key.pem, removing ...
I0503 17:42:15.651472  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/key.pem
I0503 17:42:15.651490  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/key.pem --> /home/princewillopah/.minikube/key.pem (1671 bytes)
I0503 17:42:15.651532  687371 provision.go:117] generating server cert: /home/princewillopah/.minikube/machines/server.pem ca-key=/home/princewillopah/.minikube/certs/ca.pem private-key=/home/princewillopah/.minikube/certs/ca-key.pem org=princewillopah.multinode-demo-m03 san=[127.0.0.1 192.168.67.4 localhost minikube multinode-demo-m03]
I0503 17:42:15.889168  687371 provision.go:177] copyRemoteCerts
I0503 17:42:15.889247  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0503 17:42:15.889288  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:15.921247  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32817 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m03/id_rsa Username:docker}
I0503 17:42:16.014892  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0503 17:42:16.044589  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/machines/server.pem --> /etc/docker/server.pem (1241 bytes)
I0503 17:42:16.074571  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0503 17:42:16.105172  687371 provision.go:87] duration metric: took 475.098158ms to configureAuth
I0503 17:42:16.105196  687371 ubuntu.go:193] setting minikube options for container-runtime
I0503 17:42:16.105485  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:16.105574  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:16.136575  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:16.136792  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32817 <nil> <nil>}
I0503 17:42:16.136800  687371 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0503 17:42:16.261467  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0503 17:42:16.261483  687371 ubuntu.go:71] root file system type: overlay
I0503 17:42:16.261594  687371 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0503 17:42:16.261651  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:16.281437  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:16.281681  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32817 <nil> <nil>}
I0503 17:42:16.281742  687371 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.67.2"
Environment="NO_PROXY=192.168.67.2,192.168.67.3"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0503 17:42:16.422242  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.67.2
Environment=NO_PROXY=192.168.67.2,192.168.67.3


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0503 17:42:16.422320  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:16.440548  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:16.440731  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32817 <nil> <nil>}
I0503 17:42:16.440744  687371 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0503 17:42:17.557641  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:59.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-03 17:42:16.413088187 +0000
@@ -1,46 +1,51 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Environment=NO_PROXY=192.168.67.2
+Environment=NO_PROXY=192.168.67.2,192.168.67.3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0503 17:42:17.557663  687371 machine.go:97] duration metric: took 5.47214084s to provisionDockerMachine
I0503 17:42:17.557672  687371 client.go:171] duration metric: took 12.049407825s to LocalClient.Create
I0503 17:42:17.557684  687371 start.go:167] duration metric: took 12.049446304s to libmachine.API.Create "multinode-demo"
I0503 17:42:17.557689  687371 start.go:293] postStartSetup for "multinode-demo-m03" (driver="docker")
I0503 17:42:17.557698  687371 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0503 17:42:17.557762  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0503 17:42:17.557797  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:17.581219  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32817 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m03/id_rsa Username:docker}
I0503 17:42:17.676645  687371 ssh_runner.go:195] Run: cat /etc/os-release
I0503 17:42:17.682331  687371 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0503 17:42:17.682357  687371 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0503 17:42:17.682406  687371 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0503 17:42:17.682413  687371 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0503 17:42:17.682423  687371 filesync.go:126] Scanning /home/princewillopah/.minikube/addons for local assets ...
I0503 17:42:17.682474  687371 filesync.go:126] Scanning /home/princewillopah/.minikube/files for local assets ...
I0503 17:42:17.682488  687371 start.go:296] duration metric: took 124.794259ms for postStartSetup
I0503 17:42:17.682778  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo-m03
I0503 17:42:17.703807  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:42:17.704238  687371 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0503 17:42:17.704288  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:17.724380  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32817 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m03/id_rsa Username:docker}
I0503 17:42:17.818353  687371 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0503 17:42:17.823830  687371 start.go:128] duration metric: took 12.317946793s to createHost
I0503 17:42:17.823846  687371 start.go:83] releasing machines lock for "multinode-demo-m03", held for 12.318090948s
I0503 17:42:17.823916  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo-m03
I0503 17:42:17.845525  687371 out.go:177] üåê  Found network options:
I0503 17:42:17.847082  687371 out.go:177]     ‚ñ™ NO_PROXY=192.168.67.2,192.168.67.3
W0503 17:42:17.848908  687371 proxy.go:119] fail to check proxy env: Error ip not in block
W0503 17:42:17.848928  687371 proxy.go:119] fail to check proxy env: Error ip not in block
W0503 17:42:17.848950  687371 proxy.go:119] fail to check proxy env: Error ip not in block
W0503 17:42:17.848957  687371 proxy.go:119] fail to check proxy env: Error ip not in block
I0503 17:42:17.849050  687371 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0503 17:42:17.849087  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:17.849101  687371 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0503 17:42:17.849199  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m03
I0503 17:42:17.870217  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32817 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m03/id_rsa Username:docker}
I0503 17:42:17.886494  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32817 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m03/id_rsa Username:docker}
I0503 17:42:17.958782  687371 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0503 17:42:18.082286  687371 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0503 17:42:18.082355  687371 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0503 17:42:18.114965  687371 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0503 17:42:18.114982  687371 start.go:494] detecting cgroup driver to use...
I0503 17:42:18.115009  687371 detect.go:199] detected "systemd" cgroup driver on host os
I0503 17:42:18.115092  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0503 17:42:18.134644  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0503 17:42:18.145974  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0503 17:42:18.157974  687371 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0503 17:42:18.158032  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0503 17:42:18.169805  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0503 17:42:18.181508  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0503 17:42:18.192626  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0503 17:42:18.204555  687371 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0503 17:42:18.214436  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0503 17:42:18.225367  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0503 17:42:18.238879  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0503 17:42:18.250907  687371 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0503 17:42:18.260436  687371 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0503 17:42:18.270093  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:18.385408  687371 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0503 17:42:18.501235  687371 start.go:494] detecting cgroup driver to use...
I0503 17:42:18.501284  687371 detect.go:199] detected "systemd" cgroup driver on host os
I0503 17:42:18.501328  687371 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0503 17:42:18.522706  687371 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0503 17:42:18.522784  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0503 17:42:18.537650  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0503 17:42:18.556643  687371 ssh_runner.go:195] Run: which cri-dockerd
I0503 17:42:18.562060  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0503 17:42:18.572592  687371 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0503 17:42:18.593344  687371 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0503 17:42:18.737997  687371 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0503 17:42:18.860887  687371 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0503 17:42:18.860921  687371 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0503 17:42:18.883413  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:19.008325  687371 ssh_runner.go:195] Run: sudo systemctl restart docker
I0503 17:42:19.616726  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0503 17:42:19.632112  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0503 17:42:19.645402  687371 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0503 17:42:19.761308  687371 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0503 17:42:19.877222  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:19.997664  687371 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0503 17:42:20.016086  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0503 17:42:20.030332  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:20.153301  687371 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0503 17:42:20.229743  687371 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0503 17:42:20.229810  687371 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0503 17:42:20.234506  687371 start.go:562] Will wait 60s for crictl version
I0503 17:42:20.234575  687371 ssh_runner.go:195] Run: which crictl
I0503 17:42:20.239352  687371 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0503 17:42:20.281923  687371 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0503 17:42:20.281987  687371 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0503 17:42:20.307419  687371 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0503 17:42:20.338771  687371 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0503 17:42:20.357792  687371 out.go:177]     ‚ñ™ env NO_PROXY=192.168.67.2
I0503 17:42:20.361447  687371 out.go:177]     ‚ñ™ env NO_PROXY=192.168.67.2,192.168.67.3
I0503 17:42:20.366626  687371 cli_runner.go:164] Run: docker network inspect multinode-demo --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0503 17:42:20.384576  687371 ssh_runner.go:195] Run: grep 192.168.67.1	host.minikube.internal$ /etc/hosts
I0503 17:42:20.388974  687371 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.67.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0503 17:42:20.400691  687371 mustload.go:65] Loading cluster: multinode-demo
I0503 17:42:20.400890  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:20.401080  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Status}}
I0503 17:42:20.419488  687371 host.go:66] Checking if "multinode-demo" exists ...
I0503 17:42:20.419781  687371 certs.go:68] Setting up /home/princewillopah/.minikube/profiles/multinode-demo for IP: 192.168.67.4
I0503 17:42:20.419787  687371 certs.go:194] generating shared ca certs ...
I0503 17:42:20.419802  687371 certs.go:226] acquiring lock for ca certs: {Name:mk3ec114f777ab0e10c44ed8952ba18450ef5a1a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:42:20.419921  687371 certs.go:235] skipping valid "minikubeCA" ca cert: /home/princewillopah/.minikube/ca.key
I0503 17:42:20.419981  687371 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/princewillopah/.minikube/proxy-client-ca.key
I0503 17:42:20.420050  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/ca-key.pem (1675 bytes)
I0503 17:42:20.420073  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/ca.pem (1099 bytes)
I0503 17:42:20.420089  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/cert.pem (1143 bytes)
I0503 17:42:20.420103  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/key.pem (1671 bytes)
I0503 17:42:20.420142  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0503 17:42:20.448099  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0503 17:42:20.475112  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0503 17:42:20.510349  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0503 17:42:20.539254  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0503 17:42:20.568430  687371 ssh_runner.go:195] Run: openssl version
I0503 17:42:20.574765  687371 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0503 17:42:20.585627  687371 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0503 17:42:20.590140  687371 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  2 17:21 /usr/share/ca-certificates/minikubeCA.pem
I0503 17:42:20.590193  687371 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0503 17:42:20.598102  687371 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0503 17:42:20.611895  687371 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0503 17:42:20.616022  687371 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0503 17:42:20.616057  687371 kubeadm.go:928] updating node {m03 192.168.67.4 8443 v1.30.0 docker false true} ...
I0503 17:42:20.616135  687371 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=multinode-demo-m03 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.67.4

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0503 17:42:20.616186  687371 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0503 17:42:20.627287  687371 binaries.go:44] Found k8s binaries, skipping transfer
I0503 17:42:20.627366  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0503 17:42:20.636929  687371 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (317 bytes)
I0503 17:42:20.657810  687371 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0503 17:42:20.679227  687371 ssh_runner.go:195] Run: grep 192.168.67.2	control-plane.minikube.internal$ /etc/hosts
I0503 17:42:20.683649  687371 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.67.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0503 17:42:20.696835  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:20.821458  687371 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0503 17:42:20.837813  687371 host.go:66] Checking if "multinode-demo" exists ...
I0503 17:42:20.838116  687371 start.go:316] joinCluster: &{Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.67.3 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.67.4 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0503 17:42:20.838233  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I0503 17:42:20.838286  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:42:20.866271  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:42:21.035849  687371 start.go:342] trying to join worker node "m03" to cluster: &{Name:m03 IP:192.168.67.4 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0503 17:42:21.035880  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token qu4th5.ralahy7jnhs7pz87 --discovery-token-ca-cert-hash sha256:c3e1d0d5f571a14031e41fdd48f3c3c812a60614dd8862a7927597fb41b23776 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=multinode-demo-m03"
I0503 17:42:22.392407  687371 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token qu4th5.ralahy7jnhs7pz87 --discovery-token-ca-cert-hash sha256:c3e1d0d5f571a14031e41fdd48f3c3c812a60614dd8862a7927597fb41b23776 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=multinode-demo-m03": (1.35650776s)
I0503 17:42:22.392432  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0503 17:42:22.665932  687371 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes multinode-demo-m03 minikube.k8s.io/updated_at=2024_05_03T17_42_22_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=multinode-demo minikube.k8s.io/primary=false
I0503 17:42:22.762148  687371 start.go:318] duration metric: took 1.924035724s to joinCluster
I0503 17:42:22.762213  687371 start.go:234] Will wait 6m0s for node &{Name:m03 IP:192.168.67.4 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0503 17:42:22.764977  687371 out.go:177] üîé  Verifying Kubernetes components...
I0503 17:42:22.762610  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:22.766148  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:22.905273  687371 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0503 17:42:22.921118  687371 kubeadm.go:576] duration metric: took 158.87031ms to wait for: map[apiserver:true system_pods:true]
I0503 17:42:22.921152  687371 node_conditions.go:102] verifying NodePressure condition ...
I0503 17:42:22.928833  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:42:22.928856  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:42:22.928869  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:42:22.928875  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:42:22.928880  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:42:22.928885  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:42:22.928890  687371 node_conditions.go:105] duration metric: took 7.732291ms to run NodePressure ...
I0503 17:42:22.928907  687371 start.go:240] waiting for startup goroutines ...
I0503 17:42:22.928936  687371 start.go:254] writing updated cluster config ...
I0503 17:42:22.932411  687371 out.go:177] 
I0503 17:42:22.934959  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:22.935136  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:42:22.937281  687371 out.go:177] üëç  Starting "multinode-demo-m04" worker node in "multinode-demo" cluster
I0503 17:42:22.939389  687371 cache.go:121] Beginning downloading kic base image for docker with docker
I0503 17:42:22.940664  687371 out.go:177] üöú  Pulling base image v0.0.43 ...
I0503 17:42:22.946600  687371 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0503 17:42:22.946634  687371 cache.go:56] Caching tarball of preloaded images
I0503 17:42:22.946686  687371 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0503 17:42:22.946780  687371 preload.go:173] Found /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0503 17:42:22.946797  687371 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0503 17:42:22.947016  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:42:22.967694  687371 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0503 17:42:22.967711  687371 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0503 17:42:22.967731  687371 cache.go:194] Successfully downloaded all kic artifacts
I0503 17:42:22.967765  687371 start.go:360] acquireMachinesLock for multinode-demo-m04: {Name:mk9c2459765e84ce2fb15e779ce15674aa095fe1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0503 17:42:22.967871  687371 start.go:364] duration metric: took 90.818¬µs to acquireMachinesLock for "multinode-demo-m04"
I0503 17:42:22.967891  687371 start.go:93] Provisioning new machine with config: &{Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.67.3 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.67.4 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m04 IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name:m04 IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0503 17:42:22.968048  687371 start.go:125] createHost starting for "m04" (driver="docker")
I0503 17:42:22.974056  687371 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0503 17:42:22.974200  687371 start.go:159] libmachine.API.Create for "multinode-demo" (driver="docker")
I0503 17:42:22.974222  687371 client.go:168] LocalClient.Create starting
I0503 17:42:22.974277  687371 main.go:141] libmachine: Reading certificate data from /home/princewillopah/.minikube/certs/ca.pem
I0503 17:42:22.974309  687371 main.go:141] libmachine: Decoding PEM data...
I0503 17:42:22.974319  687371 main.go:141] libmachine: Parsing certificate...
I0503 17:42:22.974365  687371 main.go:141] libmachine: Reading certificate data from /home/princewillopah/.minikube/certs/cert.pem
I0503 17:42:22.974450  687371 main.go:141] libmachine: Decoding PEM data...
I0503 17:42:22.974458  687371 main.go:141] libmachine: Parsing certificate...
I0503 17:42:22.974731  687371 cli_runner.go:164] Run: docker network inspect multinode-demo --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0503 17:42:22.995235  687371 network_create.go:77] Found existing network {name:multinode-demo subnet:0xc003128e40 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 67 1] mtu:1500}
I0503 17:42:22.995279  687371 kic.go:121] calculated static IP "192.168.67.5" for the "multinode-demo-m04" container
I0503 17:42:22.995400  687371 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0503 17:42:23.017576  687371 cli_runner.go:164] Run: docker volume create multinode-demo-m04 --label name.minikube.sigs.k8s.io=multinode-demo-m04 --label created_by.minikube.sigs.k8s.io=true
I0503 17:42:23.040724  687371 oci.go:103] Successfully created a docker volume multinode-demo-m04
I0503 17:42:23.040834  687371 cli_runner.go:164] Run: docker run --rm --name multinode-demo-m04-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=multinode-demo-m04 --entrypoint /usr/bin/test -v multinode-demo-m04:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0503 17:42:23.691525  687371 oci.go:107] Successfully prepared a docker volume multinode-demo-m04
I0503 17:42:23.691556  687371 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0503 17:42:23.691576  687371 kic.go:194] Starting extracting preloaded images to volume ...
I0503 17:42:23.691634  687371 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v multinode-demo-m04:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0503 17:42:27.333525  687371 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/princewillopah/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v multinode-demo-m04:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (3.641858423s)
I0503 17:42:27.333549  687371 kic.go:203] duration metric: took 3.641970247s to extract preloaded images to volume ...
W0503 17:42:27.333621  687371 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0503 17:42:27.333672  687371 oci.go:243] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0503 17:42:27.333712  687371 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0503 17:42:27.371714  687371 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname multinode-demo-m04 --name multinode-demo-m04 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=multinode-demo-m04 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=multinode-demo-m04 --network multinode-demo --ip 192.168.67.5 --volume multinode-demo-m04:/var --security-opt apparmor=unconfined --memory=2200mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0503 17:42:27.964868  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m04 --format={{.State.Running}}
I0503 17:42:28.001367  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m04 --format={{.State.Status}}
I0503 17:42:28.030262  687371 cli_runner.go:164] Run: docker exec multinode-demo-m04 stat /var/lib/dpkg/alternatives/iptables
I0503 17:42:28.116436  687371 oci.go:144] the created container "multinode-demo-m04" has a running status.
I0503 17:42:28.116456  687371 kic.go:225] Creating ssh key for kic: /home/princewillopah/.minikube/machines/multinode-demo-m04/id_rsa...
I0503 17:42:28.495558  687371 kic_runner.go:191] docker (temp): /home/princewillopah/.minikube/machines/multinode-demo-m04/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0503 17:42:28.526616  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m04 --format={{.State.Status}}
I0503 17:42:28.546756  687371 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0503 17:42:28.546768  687371 kic_runner.go:114] Args: [docker exec --privileged multinode-demo-m04 chown docker:docker /home/docker/.ssh/authorized_keys]
I0503 17:42:28.635701  687371 cli_runner.go:164] Run: docker container inspect multinode-demo-m04 --format={{.State.Status}}
I0503 17:42:28.663905  687371 machine.go:94] provisionDockerMachine start ...
I0503 17:42:28.664003  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:28.708350  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:28.708543  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32822 <nil> <nil>}
I0503 17:42:28.708549  687371 main.go:141] libmachine: About to run SSH command:
hostname
I0503 17:42:28.709115  687371 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:35808->127.0.0.1:32822: read: connection reset by peer
I0503 17:42:31.841029  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: multinode-demo-m04

I0503 17:42:31.841054  687371 ubuntu.go:169] provisioning hostname "multinode-demo-m04"
I0503 17:42:31.841112  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:31.866704  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:31.866912  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32822 <nil> <nil>}
I0503 17:42:31.866922  687371 main.go:141] libmachine: About to run SSH command:
sudo hostname multinode-demo-m04 && echo "multinode-demo-m04" | sudo tee /etc/hostname
I0503 17:42:32.009417  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: multinode-demo-m04

I0503 17:42:32.009514  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:32.026263  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:32.026535  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32822 <nil> <nil>}
I0503 17:42:32.026547  687371 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\smultinode-demo-m04' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 multinode-demo-m04/g' /etc/hosts;
			else 
				echo '127.0.1.1 multinode-demo-m04' | sudo tee -a /etc/hosts; 
			fi
		fi
I0503 17:42:32.145810  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0503 17:42:32.145828  687371 ubuntu.go:175] set auth options {CertDir:/home/princewillopah/.minikube CaCertPath:/home/princewillopah/.minikube/certs/ca.pem CaPrivateKeyPath:/home/princewillopah/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/princewillopah/.minikube/machines/server.pem ServerKeyPath:/home/princewillopah/.minikube/machines/server-key.pem ClientKeyPath:/home/princewillopah/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/princewillopah/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/princewillopah/.minikube}
I0503 17:42:32.145844  687371 ubuntu.go:177] setting up certificates
I0503 17:42:32.145864  687371 provision.go:84] configureAuth start
I0503 17:42:32.145919  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo-m04
I0503 17:42:32.170372  687371 provision.go:143] copyHostCerts
I0503 17:42:32.170420  687371 exec_runner.go:144] found /home/princewillopah/.minikube/key.pem, removing ...
I0503 17:42:32.170426  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/key.pem
I0503 17:42:32.170492  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/key.pem --> /home/princewillopah/.minikube/key.pem (1671 bytes)
I0503 17:42:32.170590  687371 exec_runner.go:144] found /home/princewillopah/.minikube/ca.pem, removing ...
I0503 17:42:32.170594  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/ca.pem
I0503 17:42:32.170615  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/ca.pem --> /home/princewillopah/.minikube/ca.pem (1099 bytes)
I0503 17:42:32.170658  687371 exec_runner.go:144] found /home/princewillopah/.minikube/cert.pem, removing ...
I0503 17:42:32.170660  687371 exec_runner.go:203] rm: /home/princewillopah/.minikube/cert.pem
I0503 17:42:32.170680  687371 exec_runner.go:151] cp: /home/princewillopah/.minikube/certs/cert.pem --> /home/princewillopah/.minikube/cert.pem (1143 bytes)
I0503 17:42:32.170721  687371 provision.go:117] generating server cert: /home/princewillopah/.minikube/machines/server.pem ca-key=/home/princewillopah/.minikube/certs/ca.pem private-key=/home/princewillopah/.minikube/certs/ca-key.pem org=princewillopah.multinode-demo-m04 san=[127.0.0.1 192.168.67.5 localhost minikube multinode-demo-m04]
I0503 17:42:32.227458  687371 provision.go:177] copyRemoteCerts
I0503 17:42:32.227517  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0503 17:42:32.227559  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:32.246450  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32822 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m04/id_rsa Username:docker}
I0503 17:42:32.342886  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0503 17:42:32.371231  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/machines/server.pem --> /etc/docker/server.pem (1241 bytes)
I0503 17:42:32.397175  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0503 17:42:32.422947  687371 provision.go:87] duration metric: took 277.071258ms to configureAuth
I0503 17:42:32.422964  687371 ubuntu.go:193] setting minikube options for container-runtime
I0503 17:42:32.423148  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:32.423192  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:32.441366  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:32.441525  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32822 <nil> <nil>}
I0503 17:42:32.441531  687371 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0503 17:42:32.561231  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0503 17:42:32.561244  687371 ubuntu.go:71] root file system type: overlay
I0503 17:42:32.561365  687371 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0503 17:42:32.561419  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:32.578504  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:32.578669  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32822 <nil> <nil>}
I0503 17:42:32.578730  687371 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.67.2"
Environment="NO_PROXY=192.168.67.2,192.168.67.3"
Environment="NO_PROXY=192.168.67.2,192.168.67.3,192.168.67.4"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0503 17:42:32.737314  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.67.2
Environment=NO_PROXY=192.168.67.2,192.168.67.3
Environment=NO_PROXY=192.168.67.2,192.168.67.3,192.168.67.4


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0503 17:42:32.737402  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:32.757027  687371 main.go:141] libmachine: Using SSH client type: native
I0503 17:42:32.757200  687371 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32822 <nil> <nil>}
I0503 17:42:32.757212  687371 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0503 17:42:33.843654  687371 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:59.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-03 17:42:32.729268832 +0000
@@ -1,46 +1,52 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
-
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=NO_PROXY=192.168.67.2
+Environment=NO_PROXY=192.168.67.2,192.168.67.3
+Environment=NO_PROXY=192.168.67.2,192.168.67.3,192.168.67.4
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0503 17:42:33.843672  687371 machine.go:97] duration metric: took 5.17975429s to provisionDockerMachine
I0503 17:42:33.843681  687371 client.go:171] duration metric: took 10.869455153s to LocalClient.Create
I0503 17:42:33.843694  687371 start.go:167] duration metric: took 10.869494836s to libmachine.API.Create "multinode-demo"
I0503 17:42:33.843700  687371 start.go:293] postStartSetup for "multinode-demo-m04" (driver="docker")
I0503 17:42:33.843709  687371 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0503 17:42:33.843764  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0503 17:42:33.843812  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:33.865067  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32822 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m04/id_rsa Username:docker}
I0503 17:42:33.960124  687371 ssh_runner.go:195] Run: cat /etc/os-release
I0503 17:42:33.963351  687371 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0503 17:42:33.963372  687371 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0503 17:42:33.963381  687371 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0503 17:42:33.963386  687371 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0503 17:42:33.963395  687371 filesync.go:126] Scanning /home/princewillopah/.minikube/addons for local assets ...
I0503 17:42:33.963447  687371 filesync.go:126] Scanning /home/princewillopah/.minikube/files for local assets ...
I0503 17:42:33.963460  687371 start.go:296] duration metric: took 119.756089ms for postStartSetup
I0503 17:42:33.963788  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo-m04
I0503 17:42:33.980810  687371 profile.go:143] Saving config to /home/princewillopah/.minikube/profiles/multinode-demo/config.json ...
I0503 17:42:33.981121  687371 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0503 17:42:33.981157  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:34.000160  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32822 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m04/id_rsa Username:docker}
I0503 17:42:34.086387  687371 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0503 17:42:34.091013  687371 start.go:128] duration metric: took 11.122949413s to createHost
I0503 17:42:34.091031  687371 start.go:83] releasing machines lock for "multinode-demo-m04", held for 11.123153055s
I0503 17:42:34.091095  687371 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" multinode-demo-m04
I0503 17:42:34.115290  687371 out.go:177] üåê  Found network options:
I0503 17:42:34.116656  687371 out.go:177]     ‚ñ™ NO_PROXY=192.168.67.2,192.168.67.3,192.168.67.4
W0503 17:42:34.117621  687371 proxy.go:119] fail to check proxy env: Error ip not in block
W0503 17:42:34.117637  687371 proxy.go:119] fail to check proxy env: Error ip not in block
W0503 17:42:34.117644  687371 proxy.go:119] fail to check proxy env: Error ip not in block
W0503 17:42:34.117666  687371 proxy.go:119] fail to check proxy env: Error ip not in block
W0503 17:42:34.117680  687371 proxy.go:119] fail to check proxy env: Error ip not in block
W0503 17:42:34.117687  687371 proxy.go:119] fail to check proxy env: Error ip not in block
I0503 17:42:34.117767  687371 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0503 17:42:34.117812  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:34.117813  687371 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0503 17:42:34.117864  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo-m04
I0503 17:42:34.145471  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32822 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m04/id_rsa Username:docker}
I0503 17:42:34.146847  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32822 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo-m04/id_rsa Username:docker}
I0503 17:42:34.337852  687371 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0503 17:42:34.369626  687371 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0503 17:42:34.369698  687371 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0503 17:42:34.402813  687371 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0503 17:42:34.402832  687371 start.go:494] detecting cgroup driver to use...
I0503 17:42:34.402857  687371 detect.go:199] detected "systemd" cgroup driver on host os
I0503 17:42:34.402940  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0503 17:42:34.419699  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0503 17:42:34.435247  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0503 17:42:34.447426  687371 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0503 17:42:34.447480  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0503 17:42:34.459380  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0503 17:42:34.472449  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0503 17:42:34.483280  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0503 17:42:34.494067  687371 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0503 17:42:34.504308  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0503 17:42:34.514413  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0503 17:42:34.524849  687371 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0503 17:42:34.535034  687371 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0503 17:42:34.543458  687371 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0503 17:42:34.551349  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:34.661717  687371 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0503 17:42:34.767644  687371 start.go:494] detecting cgroup driver to use...
I0503 17:42:34.767698  687371 detect.go:199] detected "systemd" cgroup driver on host os
I0503 17:42:34.767763  687371 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0503 17:42:34.785765  687371 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0503 17:42:34.785847  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0503 17:42:34.799800  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0503 17:42:34.822280  687371 ssh_runner.go:195] Run: which cri-dockerd
I0503 17:42:34.826232  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0503 17:42:34.836926  687371 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0503 17:42:34.857931  687371 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0503 17:42:34.985212  687371 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0503 17:42:35.098026  687371 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0503 17:42:35.098073  687371 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0503 17:42:35.119907  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:35.242934  687371 ssh_runner.go:195] Run: sudo systemctl restart docker
I0503 17:42:35.806321  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0503 17:42:35.818795  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0503 17:42:35.832975  687371 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0503 17:42:35.961578  687371 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0503 17:42:36.066815  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:36.177891  687371 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0503 17:42:36.192943  687371 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0503 17:42:36.204364  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:36.318640  687371 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0503 17:42:36.403938  687371 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0503 17:42:36.404033  687371 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0503 17:42:36.407864  687371 start.go:562] Will wait 60s for crictl version
I0503 17:42:36.407915  687371 ssh_runner.go:195] Run: which crictl
I0503 17:42:36.412240  687371 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0503 17:42:36.450274  687371 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0503 17:42:36.450339  687371 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0503 17:42:36.478658  687371 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0503 17:42:36.507258  687371 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0503 17:42:36.508513  687371 out.go:177]     ‚ñ™ env NO_PROXY=192.168.67.2
I0503 17:42:36.510343  687371 out.go:177]     ‚ñ™ env NO_PROXY=192.168.67.2,192.168.67.3
I0503 17:42:36.511490  687371 out.go:177]     ‚ñ™ env NO_PROXY=192.168.67.2,192.168.67.3,192.168.67.4
I0503 17:42:36.512569  687371 cli_runner.go:164] Run: docker network inspect multinode-demo --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0503 17:42:36.532442  687371 ssh_runner.go:195] Run: grep 192.168.67.1	host.minikube.internal$ /etc/hosts
I0503 17:42:36.536687  687371 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.67.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0503 17:42:36.548477  687371 mustload.go:65] Loading cluster: multinode-demo
I0503 17:42:36.548704  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:36.548908  687371 cli_runner.go:164] Run: docker container inspect multinode-demo --format={{.State.Status}}
I0503 17:42:36.565636  687371 host.go:66] Checking if "multinode-demo" exists ...
I0503 17:42:36.565866  687371 certs.go:68] Setting up /home/princewillopah/.minikube/profiles/multinode-demo for IP: 192.168.67.5
I0503 17:42:36.565870  687371 certs.go:194] generating shared ca certs ...
I0503 17:42:36.565889  687371 certs.go:226] acquiring lock for ca certs: {Name:mk3ec114f777ab0e10c44ed8952ba18450ef5a1a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0503 17:42:36.565990  687371 certs.go:235] skipping valid "minikubeCA" ca cert: /home/princewillopah/.minikube/ca.key
I0503 17:42:36.566020  687371 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/princewillopah/.minikube/proxy-client-ca.key
I0503 17:42:36.566096  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/ca-key.pem (1675 bytes)
I0503 17:42:36.566116  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/ca.pem (1099 bytes)
I0503 17:42:36.566134  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/cert.pem (1143 bytes)
I0503 17:42:36.566149  687371 certs.go:484] found cert: /home/princewillopah/.minikube/certs/key.pem (1671 bytes)
I0503 17:42:36.566187  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0503 17:42:36.595694  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0503 17:42:36.631187  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0503 17:42:36.658516  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0503 17:42:36.687212  687371 ssh_runner.go:362] scp /home/princewillopah/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0503 17:42:36.725167  687371 ssh_runner.go:195] Run: openssl version
I0503 17:42:36.732493  687371 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0503 17:42:36.743530  687371 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0503 17:42:36.747673  687371 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  2 17:21 /usr/share/ca-certificates/minikubeCA.pem
I0503 17:42:36.747723  687371 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0503 17:42:36.755044  687371 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0503 17:42:36.764848  687371 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0503 17:42:36.768209  687371 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0503 17:42:36.768239  687371 kubeadm.go:928] updating node {m04 192.168.67.5 8443 v1.30.0 docker false true} ...
I0503 17:42:36.768321  687371 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=multinode-demo-m04 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.67.5

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0503 17:42:36.768370  687371 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0503 17:42:36.777017  687371 binaries.go:44] Found k8s binaries, skipping transfer
I0503 17:42:36.777066  687371 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0503 17:42:36.785773  687371 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (317 bytes)
I0503 17:42:36.808002  687371 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0503 17:42:36.829023  687371 ssh_runner.go:195] Run: grep 192.168.67.2	control-plane.minikube.internal$ /etc/hosts
I0503 17:42:36.832447  687371 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.67.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0503 17:42:36.844704  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:36.966230  687371 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0503 17:42:36.981276  687371 host.go:66] Checking if "multinode-demo" exists ...
I0503 17:42:36.981537  687371 start.go:316] joinCluster: &{Name:multinode-demo KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:multinode-demo Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.67.3 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.67.4 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m04 IP:192.168.67.5 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/princewillopah:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0503 17:42:36.981668  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I0503 17:42:36.981721  687371 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" multinode-demo
I0503 17:42:37.018857  687371 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32807 SSHKeyPath:/home/princewillopah/.minikube/machines/multinode-demo/id_rsa Username:docker}
I0503 17:42:37.200434  687371 start.go:342] trying to join worker node "m04" to cluster: &{Name:m04 IP:192.168.67.5 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0503 17:42:37.200505  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 7h2r2p.rzbvn7uib9gk16fm --discovery-token-ca-cert-hash sha256:c3e1d0d5f571a14031e41fdd48f3c3c812a60614dd8862a7927597fb41b23776 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=multinode-demo-m04"
I0503 17:42:38.735344  687371 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 7h2r2p.rzbvn7uib9gk16fm --discovery-token-ca-cert-hash sha256:c3e1d0d5f571a14031e41fdd48f3c3c812a60614dd8862a7927597fb41b23776 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=multinode-demo-m04": (1.534817162s)
I0503 17:42:38.735369  687371 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0503 17:42:39.001602  687371 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes multinode-demo-m04 minikube.k8s.io/updated_at=2024_05_03T17_42_39_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=multinode-demo minikube.k8s.io/primary=false
I0503 17:42:39.094181  687371 start.go:318] duration metric: took 2.112652561s to joinCluster
I0503 17:42:39.094248  687371 start.go:234] Will wait 6m0s for node &{Name:m04 IP:192.168.67.5 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0503 17:42:39.096512  687371 out.go:177] üîé  Verifying Kubernetes components...
I0503 17:42:39.094579  687371 config.go:182] Loaded profile config "multinode-demo": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0503 17:42:39.097615  687371 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0503 17:42:39.214892  687371 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0503 17:42:39.229064  687371 kubeadm.go:576] duration metric: took 134.787026ms to wait for: map[apiserver:true system_pods:true]
I0503 17:42:39.229083  687371 node_conditions.go:102] verifying NodePressure condition ...
I0503 17:42:39.232612  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:42:39.232626  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:42:39.232635  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:42:39.232638  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:42:39.232640  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:42:39.232643  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:42:39.232646  687371 node_conditions.go:122] node storage ephemeral capacity is 162406320Ki
I0503 17:42:39.232654  687371 node_conditions.go:123] node cpu capacity is 2
I0503 17:42:39.232657  687371 node_conditions.go:105] duration metric: took 3.570735ms to run NodePressure ...
I0503 17:42:39.232667  687371 start.go:240] waiting for startup goroutines ...
I0503 17:42:39.232686  687371 start.go:254] writing updated cluster config ...
I0503 17:42:39.232970  687371 ssh_runner.go:195] Run: rm -f paused
I0503 17:42:39.288450  687371 start.go:600] kubectl: 1.29.0-eks-5e0fdde, cluster: 1.30.0 (minor skew: 1)
I0503 17:42:39.319464  687371 out.go:177] üèÑ  Done! kubectl is now configured to use "multinode-demo" cluster and "default" namespace by default


==> Docker <==
May 26 21:35:48 multinode-demo cri-dockerd[1234]: time="2024-05-26T21:35:48Z" level=error msg="error getting RW layer size for container ID 'f3f2f7ecc40d797a701fc3c08e12346f921fbcb3eca81d057863b2049cfeaa67': Error response from daemon: No such container: f3f2f7ecc40d797a701fc3c08e12346f921fbcb3eca81d057863b2049cfeaa67"
May 26 21:35:48 multinode-demo cri-dockerd[1234]: time="2024-05-26T21:35:48Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'f3f2f7ecc40d797a701fc3c08e12346f921fbcb3eca81d057863b2049cfeaa67'"
May 26 22:13:56 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:13:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eaa239b7c01702d06c236fa907062a1cdfd617e54d2e58236be22c6c6ccb5187/resolv.conf as [nameserver 10.96.0.10 search xxx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 26 22:13:56 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:13:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d4be03f083d2a5f2a3bd4eff36334b1e63bc57ec4490a5d9446d1d0c48564b42/resolv.conf as [nameserver 10.96.0.10 search xxx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 26 22:13:56 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:13:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c3f285d5bdf6ff355313d031513fee817a356273b1981b8ce41eb76ccc6ae020/resolv.conf as [nameserver 10.96.0.10 search xxx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 26 22:13:56 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:13:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/86c2071c682de464f5cbe21c7b85eb9c501e6abfc303bbf9d9b1a88057e153ac/resolv.conf as [nameserver 10.96.0.10 search xxx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 26 22:13:59 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:13:59Z" level=info msg="Stop pulling image princewillopah/new-backend:v1: Status: Downloaded newer image for princewillopah/new-backend:v1"
May 26 22:14:02 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:14:02Z" level=info msg="Stop pulling image princewillopah/new-frontend:v1: Status: Downloaded newer image for princewillopah/new-frontend:v1"
May 26 22:14:03 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:14:03Z" level=info msg="Stop pulling image princewillopah/new-frontend:v1: Status: Image is up to date for princewillopah/new-frontend:v1"
May 26 22:14:03 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:14:03Z" level=info msg="Stop pulling image princewillopah/new-backend:v1: Status: Image is up to date for princewillopah/new-backend:v1"
May 26 22:39:12 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:39:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4d25431c119f6d0bb6e8d0d92c409abf94d4d9095b31647bb6cdc8281a1ca8dd/resolv.conf as [nameserver 10.96.0.10 search xxx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 26 22:39:12 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:39:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7bde379363f471b6965ee954b7f85d9723989b9e93422b88686e378fdcf2f665/resolv.conf as [nameserver 10.96.0.10 search xxx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 26 22:39:13 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:39:13Z" level=info msg="Stop pulling image princewillopah/new-backend:v2: Status: Downloaded newer image for princewillopah/new-backend:v2"
May 26 22:39:15 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:39:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4dff90bcbd29e31fa07c7d5512cbfb28ce754a54033728bcc7762131c7c06bd0/resolv.conf as [nameserver 10.96.0.10 search xxx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 26 22:39:15 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:39:15Z" level=info msg="Stop pulling image princewillopah/new-frontend:v2: Status: Downloaded newer image for princewillopah/new-frontend:v2"
May 26 22:39:16 multinode-demo dockerd[1022]: time="2024-05-26T22:39:16.029298016Z" level=info msg="ignoring event" container=e3d492b2ebfce0bd2802dd6bea60337e1917c7137c2702c5ea2d0bbe07219eac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 26 22:39:16 multinode-demo dockerd[1022]: time="2024-05-26T22:39:16.180561525Z" level=info msg="ignoring event" container=86c2071c682de464f5cbe21c7b85eb9c501e6abfc303bbf9d9b1a88057e153ac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 26 22:39:16 multinode-demo cri-dockerd[1234]: time="2024-05-26T22:39:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f9592bb9bd7dfeee3d08e4ab6245d3be853e49cd8774d8f4c66bf92f006b4cf9/resolv.conf as [nameserver 10.96.0.10 search xxx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 26 22:39:18 multinode-demo dockerd[1022]: time="2024-05-26T22:39:18.138175439Z" level=info msg="ignoring event" container=c310e3f982d8f8f34e841ee594ddc45763551a82c275b22501e3d0bbb2ead399 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 26 22:39:18 multinode-demo dockerd[1022]: time="2024-05-26T22:39:18.266835560Z" level=info msg="ignoring event" container=d4be03f083d2a5f2a3bd4eff36334b1e63bc57ec4490a5d9446d1d0c48564b42 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 26 22:39:44 multinode-demo dockerd[1022]: time="2024-05-26T22:39:44.785647317Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=42c00c972a31cebcd5f95c4df99bde2e3b98f6decb9dbeea258164ef0cca58df spanID=179175ebc8b47ac7 traceID=506348742d7ac75631fd175300d3c944
May 26 22:39:44 multinode-demo dockerd[1022]: time="2024-05-26T22:39:44.831794473Z" level=info msg="ignoring event" container=42c00c972a31cebcd5f95c4df99bde2e3b98f6decb9dbeea258164ef0cca58df module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 26 22:39:44 multinode-demo dockerd[1022]: time="2024-05-26T22:39:44.937759641Z" level=info msg="ignoring event" container=eaa239b7c01702d06c236fa907062a1cdfd617e54d2e58236be22c6c6ccb5187 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 26 22:39:45 multinode-demo dockerd[1022]: time="2024-05-26T22:39:45.861923802Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=4b7954fa86f22076aee039ec8a7722e3eb1b56c3d8584f7c3d3ef0b965432ec3 spanID=7377dbd755c06ab0 traceID=b3a2c93da8ab33d6c9e687d6522f0d4c
May 26 22:39:45 multinode-demo dockerd[1022]: time="2024-05-26T22:39:45.910562210Z" level=info msg="ignoring event" container=4b7954fa86f22076aee039ec8a7722e3eb1b56c3d8584f7c3d3ef0b965432ec3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 26 22:39:46 multinode-demo dockerd[1022]: time="2024-05-26T22:39:46.044124235Z" level=info msg="ignoring event" container=c3f285d5bdf6ff355313d031513fee817a356273b1981b8ce41eb76ccc6ae020 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 16:40:38 multinode-demo dockerd[1022]: time="2024-05-27T16:40:38.410653004Z" level=info msg="ignoring event" container=fa1cbd84cf546ecdaf79e13860dda5f426a840e11267d46a76d2b6aa65e187d8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 16:40:38 multinode-demo dockerd[1022]: time="2024-05-27T16:40:38.428117412Z" level=info msg="ignoring event" container=a860368416f5ddcf9db257cd548441cd010bcf73b75fed146ba918ba733248ca module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 16:40:38 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:40:38Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"frontend-deployment-5878dc99d9-2blpd_xxx\": unexpected command output nsenter: cannot open /proc/240071/ns/net: No such file or directory\n with error: exit status 1"
May 27 16:40:38 multinode-demo dockerd[1022]: time="2024-05-27T16:40:38.670577823Z" level=info msg="ignoring event" container=f9592bb9bd7dfeee3d08e4ab6245d3be853e49cd8774d8f4c66bf92f006b4cf9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 16:40:38 multinode-demo dockerd[1022]: time="2024-05-27T16:40:38.678893633Z" level=info msg="ignoring event" container=7bde379363f471b6965ee954b7f85d9723989b9e93422b88686e378fdcf2f665 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 16:41:08 multinode-demo dockerd[1022]: time="2024-05-27T16:41:08.241710385Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=7f21744eb3e5a481bb974ecb4811a92c8f12e25788abb164aac9b82a782b6916 spanID=b079f53b80d3d327 traceID=2d67e490e12b6b93f4c4b51bd2ad6668
May 27 16:41:08 multinode-demo dockerd[1022]: time="2024-05-27T16:41:08.259309453Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=e419561101caf026e7fc8ef7cb84c8f469abab9c8b092989c81af7b7274b65dc spanID=80178d8952ee2975 traceID=cf6adbc972190cfc35b4ccefc3c940a9
May 27 16:41:08 multinode-demo dockerd[1022]: time="2024-05-27T16:41:08.305472226Z" level=info msg="ignoring event" container=7f21744eb3e5a481bb974ecb4811a92c8f12e25788abb164aac9b82a782b6916 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 16:41:08 multinode-demo dockerd[1022]: time="2024-05-27T16:41:08.350313664Z" level=info msg="ignoring event" container=e419561101caf026e7fc8ef7cb84c8f469abab9c8b092989c81af7b7274b65dc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 16:41:08 multinode-demo dockerd[1022]: time="2024-05-27T16:41:08.456057560Z" level=info msg="ignoring event" container=4d25431c119f6d0bb6e8d0d92c409abf94d4d9095b31647bb6cdc8281a1ca8dd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 16:41:08 multinode-demo dockerd[1022]: time="2024-05-27T16:41:08.535549189Z" level=info msg="ignoring event" container=4dff90bcbd29e31fa07c7d5512cbfb28ce754a54033728bcc7762131c7c06bd0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 27 16:43:24 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/520df59fc6a9cc403934b36dccb7e1e402bc360ed9fb416eba4aae416e1e9c75/resolv.conf as [nameserver 10.96.0.10 search online-boutique.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 16:43:24 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/de52692af733f0f78f522f720bf07bc6071b9768b9fd528c03aab557cb162bc0/resolv.conf as [nameserver 10.96.0.10 search online-boutique.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 16:43:24 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cc47ef98a33b170d10bc3adf9d6d616bb506bcd9fc384ac4a18cc139547c9f5f/resolv.conf as [nameserver 10.96.0.10 search online-boutique.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 16:43:24 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7f517d4d0117e79ed68f64c639d75290c939a7d2aff6cf76172b965f510e0e5a/resolv.conf as [nameserver 10.96.0.10 search online-boutique.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 16:43:24 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/54a3089030a9ab1fe81331fe049b71d19839addea702930a9635480dde83a702/resolv.conf as [nameserver 10.96.0.10 search online-boutique.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 16:43:25 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/870c14cc6d481a2ce5df61a0b2963621889419ff13890d4ba50e85e65ccc6f8b/resolv.conf as [nameserver 10.96.0.10 search online-boutique.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 27 16:43:30 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:30Z" level=info msg="Stop pulling image gcr.io/google-samples/microservices-demo/adservice:v0.8.0: Status: Downloaded newer image for gcr.io/google-samples/microservices-demo/adservice:v0.8.0"
May 27 16:43:33 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:33Z" level=info msg="Stop pulling image gcr.io/google-samples/microservices-demo/cartservice:v0.8.0: Status: Downloaded newer image for gcr.io/google-samples/microservices-demo/cartservice:v0.8.0"
May 27 16:43:36 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:36Z" level=info msg="Stop pulling image gcr.io/google-samples/microservices-demo/checkoutservice:v0.8.0: Status: Downloaded newer image for gcr.io/google-samples/microservices-demo/checkoutservice:v0.8.0"
May 27 16:43:46 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:46Z" level=info msg="Pulling image gcr.io/google-samples/microservices-demo/currencyservice:v0.8.0: 0fbcd846e4e0: Extracting [=====================>                             ]  8.716MB/20.01MB"
May 27 16:43:50 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:50Z" level=info msg="Stop pulling image gcr.io/google-samples/microservices-demo/currencyservice:v0.8.0: Status: Downloaded newer image for gcr.io/google-samples/microservices-demo/currencyservice:v0.8.0"
May 27 16:43:54 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:43:54Z" level=info msg="Stop pulling image gcr.io/google-samples/microservices-demo/frontend:v0.8.0: Status: Downloaded newer image for gcr.io/google-samples/microservices-demo/frontend:v0.8.0"
May 27 16:44:05 multinode-demo cri-dockerd[1234]: time="2024-05-27T16:44:05Z" level=info msg="Stop pulling image gcr.io/google-samples/microservices-demo/emailservice:v0.8.0: Status: Downloaded newer image for gcr.io/google-samples/microservices-demo/emailservice:v0.8.0"
May 30 05:46:03 multinode-demo cri-dockerd[1234]: time="2024-05-30T05:46:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0fb3bd50a6d4614f91ccb40fd8244fb68f8b12cf9e9bfbe0311b8847e3193697/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 30 05:46:03 multinode-demo cri-dockerd[1234]: time="2024-05-30T05:46:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b652055df8b1a68138fa5fdab56ff8081489049cb51e68d1901bf86616cd4fef/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 30 05:46:03 multinode-demo dockerd[1022]: time="2024-05-30T05:46:03.237266541Z" level=warning msg="reference for unknown type: " digest="sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334" spanID=24a5c4b19d8bacb4 traceID=81d3466ac1ed12c8addb5b7ac93d7521
May 30 05:46:04 multinode-demo cri-dockerd[1234]: time="2024-05-30T05:46:04Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334"
May 30 05:46:04 multinode-demo cri-dockerd[1234]: time="2024-05-30T05:46:04Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334"
May 30 05:46:04 multinode-demo dockerd[1022]: time="2024-05-30T05:46:04.952314596Z" level=info msg="ignoring event" container=a0f6f1660864a07da05db79b30ee26c62e881eac863f43ac111dc702f26ea536 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 30 05:46:05 multinode-demo dockerd[1022]: time="2024-05-30T05:46:05.049459793Z" level=info msg="ignoring event" container=d85c684580740cbc43b7d7171b7956ffac9d5947989f9c006a0ce6559caea561 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 30 05:46:05 multinode-demo dockerd[1022]: time="2024-05-30T05:46:05.460707057Z" level=info msg="ignoring event" container=dc249b0fd6231734f7882ef498651e6183d90ef370c20d3181ce86641b9d256b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 30 05:46:06 multinode-demo dockerd[1022]: time="2024-05-30T05:46:06.404989015Z" level=info msg="ignoring event" container=b652055df8b1a68138fa5fdab56ff8081489049cb51e68d1901bf86616cd4fef module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 30 05:46:07 multinode-demo dockerd[1022]: time="2024-05-30T05:46:07.572586098Z" level=info msg="ignoring event" container=0fb3bd50a6d4614f91ccb40fd8244fb68f8b12cf9e9bfbe0311b8847e3193697 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                                              CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
dc249b0fd6231       b29d748098e32                                                                                                                      7 minutes ago       Exited              patch                     1                   0fb3bd50a6d46       ingress-nginx-admission-patch-cmbnr
d85c684580740       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:44d1d0e9f19c63f58b380c5fddaca7cf22c7cee564adeff365225a5df5ef3334         7 minutes ago       Exited              create                    0                   b652055df8b1a       ingress-nginx-admission-create-fv95t
4ea46b81c0182       gcr.io/google-samples/microservices-demo/emailservice@sha256:53d0416f79e3d4ba8d2092d7c48880375e3398f6e086996aa12b2e68d0a04976      2 days ago          Running             server                    0                   870c14cc6d481       emailservice-859f88dbd8-b6884
70637674be420       gcr.io/google-samples/microservices-demo/frontend@sha256:048d9344b759afffd52f4585fb11ee5842f013857eec1136b60973f9ee2fc8f0          2 days ago          Running             server                    0                   54a3089030a9a       frontend-84c66d58cd-2f26m
b300fe62162a5       gcr.io/google-samples/microservices-demo/currencyservice@sha256:bd0fa063169b5fab3461c2b21022714bf5c1429b25334e667700a41e5bd2ab30   2 days ago          Running             server                    0                   7f517d4d0117e       currencyservice-86666df7bb-qzbzf
48f6999d40a24       gcr.io/google-samples/microservices-demo/checkoutservice@sha256:c703e33e1c7ef6e0cdfee582b4b948159b964d502d352d14ed452df2e5a40734   2 days ago          Running             server                    0                   cc47ef98a33b1       checkoutservice-64df5f5d7b-4m8n7
dcc84ef021d07       gcr.io/google-samples/microservices-demo/cartservice@sha256:49a71db4bf4cf0a4c0e789f3cb092950a53686087e619a8ad6b02c679ba135e3       2 days ago          Running             server                    0                   de52692af733f       cartservice-79795bd44f-cgckf
ecfb72203caeb       gcr.io/google-samples/microservices-demo/adservice@sha256:45fb8ed886902c0c49e044b1f8870fad61c1022fa23c4943098302a8f1c5b75f         2 days ago          Running             server                    0                   520df59fc6a9c       adservice-5d4f9d9758-zqq9w
87bf5b774c7a8       a24c7c057ec87                                                                                                                      9 days ago          Running             metrics-server            1                   29c17204381e3       metrics-server-c59844bb4-zmch4
72d603d1645f1       6e38f40d628db                                                                                                                      9 days ago          Running             storage-provisioner       3                   218e5ca0ac02f       storage-provisioner
95088da314c96       a24c7c057ec87                                                                                                                      9 days ago          Running             metrics-server            1                   49b2b86575c71       metrics-server-7ffbc6d68-qxbpw
443502d90dd2c       6e38f40d628db                                                                                                                      11 days ago         Exited              storage-provisioner       2                   218e5ca0ac02f       storage-provisioner
df31735c26a44       registry.k8s.io/autoscaling/vpa-admission-controller@sha256:bd81641d0f6302a08b32293f97b419b67a7b7f4d977ad7109fef06f485a66696       3 weeks ago         Running             admission-controller      0                   c286f4cb233c8       vpa-admission-controller-cb5f8dbc8-grhg2
656e04ec6bd33       registry.k8s.io/autoscaling/vpa-recommender@sha256:72bd6e7df2c54320c2b69a7e510b867a6602fdce4112ecde36ae7521a460bdcf                3 weeks ago         Running             recommender               0                   cbf077a072fbd       vpa-recommender-58886885f-dr5sv
068a4ca52f0cb       registry.k8s.io/autoscaling/vpa-updater@sha256:7ac4deb37e2e46de02396b78175f38c8d9b86ff7cf8df6f6dc09dbdbb9d8998c                    3 weeks ago         Running             updater                   0                   3aa8bb2497d0e       vpa-updater-5f44c47fd6-j95b8
a334269c2a653       registry.k8s.io/metrics-server/metrics-server@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a              3 weeks ago         Exited              metrics-server            0                   49b2b86575c71       metrics-server-7ffbc6d68-qxbpw
f9b5c71d5c3e6       registry.k8s.io/metrics-server/metrics-server@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a              3 weeks ago         Exited              metrics-server            0                   29c17204381e3       metrics-server-c59844bb4-zmch4
cd8eabdbc2a9c       cbb01a7bd410d                                                                                                                      3 weeks ago         Running             coredns                   1                   ca7e4b94c3754       coredns-7db6d8ff4d-66mdv
316a57c597c61       kindest/kindnetd@sha256:61f9956af8019caf6dcc4d39b31857b868aaab80521432ddcc216b805c4f7988                                           3 weeks ago         Running             kindnet-cni               0                   450c35f4e75fc       kindnet-pc4pj
466cc02ba54de       a0bf559e280cf                                                                                                                      3 weeks ago         Running             kube-proxy                0                   2faaea0cd7e2b       kube-proxy-x522v
280a80d887c33       cbb01a7bd410d                                                                                                                      3 weeks ago         Exited              coredns                   0                   d134cba92ef61       coredns-7db6d8ff4d-66mdv
9389a26750cc2       259c8277fcbbc                                                                                                                      3 weeks ago         Running             kube-scheduler            0                   c500e534c8bc9       kube-scheduler-multinode-demo
c20f0524e5503       c42f13656d0b2                                                                                                                      3 weeks ago         Running             kube-apiserver            0                   ddb025dee150b       kube-apiserver-multinode-demo
4579c55d33586       c7aad43836fa5                                                                                                                      3 weeks ago         Running             kube-controller-manager   0                   669b5a2a061c4       kube-controller-manager-multinode-demo
c04ca8e7ae093       3861cfcd7c04c                                                                                                                      3 weeks ago         Running             etcd                      0                   90c3881ec53ff       etcd-multinode-demo


==> coredns [280a80d887c3] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: network is unreachable
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = bfa258e3dfcd8004ab6c7d60772766a595ee209e49c62e6ae56bd911a145318b327e0c73bbccac30667047dafea6a8c1149027cea85d58a2246677e8ec1caab2
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] plugin/health: Going into lameduck mode for 5s
[INFO] 127.0.0.1:60080 - 29369 "HINFO IN 294392254648428398.2636106369483336548. udp 56 false 512" - - 0 5.000277435s
[ERROR] plugin/errors: 2 294392254648428398.2636106369483336548. HINFO: dial udp 192.168.67.1:53: connect: network is unreachable
[INFO] 127.0.0.1:47280 - 35973 "HINFO IN 294392254648428398.2636106369483336548. udp 56 false 512" - - 0 5.000080673s
[ERROR] plugin/errors: 2 294392254648428398.2636106369483336548. HINFO: dial udp 192.168.67.1:53: connect: network is unreachable


==> coredns [cd8eabdbc2a9] <==
[INFO] 10.244.0.147:55089 - 61034 "AAAA IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.xxx.svc.cluster.local. udp 82 false 512" NXDOMAIN qr,aa,rd 175 0.000070771s
[INFO] 10.244.0.147:55089 - 60762 "A IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.xxx.svc.cluster.local. udp 82 false 512" NXDOMAIN qr,aa,rd 175 0.000157922s
[INFO] 10.244.0.147:34564 - 22128 "A IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000063527s
[INFO] 10.244.0.147:34564 - 22336 "AAAA IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000153124s
[INFO] 10.244.0.147:42740 - 24949 "A IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000063562s
[INFO] 10.244.0.147:42740 - 25156 "AAAA IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000145717s
[INFO] 10.244.0.147:48724 - 12490 "AAAA IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 390 0.000054099s
[INFO] 10.244.0.147:48724 - 12036 "A IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000032423s
[INFO] 10.244.0.149:45614 - 56551 "A IN ac-kg3i329-shard-00-01.z3jjaj2.mongodb.net.xxx.svc.cluster.local. udp 82 false 512" NXDOMAIN qr,aa,rd 175 0.000146584s
[INFO] 10.244.0.149:45614 - 56866 "AAAA IN ac-kg3i329-shard-00-01.z3jjaj2.mongodb.net.xxx.svc.cluster.local. udp 82 false 512" NXDOMAIN qr,aa,rd 175 0.000526083s
[INFO] 10.244.0.149:41995 - 32215 "AAAA IN ac-kg3i329-shard-00-01.z3jjaj2.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000071767s
[INFO] 10.244.0.149:41995 - 31966 "A IN ac-kg3i329-shard-00-01.z3jjaj2.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000161965s
[INFO] 10.244.0.149:51432 - 15613 "AAAA IN ac-kg3i329-shard-00-01.z3jjaj2.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000079474s
[INFO] 10.244.0.149:51432 - 15153 "A IN ac-kg3i329-shard-00-01.z3jjaj2.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000148798s
[INFO] 10.244.0.149:42471 - 25418 "A IN ac-kg3i329-shard-00-01.z3jjaj2.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 344 0.000055656s
[INFO] 10.244.0.149:42471 - 25622 "AAAA IN ac-kg3i329-shard-00-01.z3jjaj2.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 391 0.000266814s
[INFO] 10.244.0.149:50410 - 46766 "A IN ac-kg3i329-shard-00-02.z3jjaj2.mongodb.net.xxx.svc.cluster.local. udp 82 false 512" NXDOMAIN qr,aa,rd 175 0.000065041s
[INFO] 10.244.0.149:50410 - 47132 "AAAA IN ac-kg3i329-shard-00-02.z3jjaj2.mongodb.net.xxx.svc.cluster.local. udp 82 false 512" NXDOMAIN qr,aa,rd 175 0.000042806s
[INFO] 10.244.0.149:47391 - 57053 "A IN ac-kg3i329-shard-00-02.z3jjaj2.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000036606s
[INFO] 10.244.0.149:47391 - 57340 "AAAA IN ac-kg3i329-shard-00-02.z3jjaj2.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000058521s
[INFO] 10.244.0.149:35464 - 57490 "AAAA IN ac-kg3i329-shard-00-02.z3jjaj2.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000067933s
[INFO] 10.244.0.149:35464 - 57243 "A IN ac-kg3i329-shard-00-02.z3jjaj2.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000433716s
[INFO] 10.244.0.149:53925 - 22957 "A IN ac-kg3i329-shard-00-02.z3jjaj2.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000079846s
[INFO] 10.244.0.149:53925 - 23237 "AAAA IN ac-kg3i329-shard-00-02.z3jjaj2.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 390 0.000090809s
[INFO] 10.244.0.149:55434 - 55913 "A IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.xxx.svc.cluster.local. udp 82 false 512" NXDOMAIN qr,aa,rd 175 0.000079699s
[INFO] 10.244.0.149:55434 - 56227 "AAAA IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.xxx.svc.cluster.local. udp 82 false 512" NXDOMAIN qr,aa,rd 175 0.000197541s
[INFO] 10.244.0.149:48322 - 55108 "A IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.00003728s
[INFO] 10.244.0.149:48322 - 55404 "AAAA IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.00013277s
[INFO] 10.244.0.149:44331 - 35866 "AAAA IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000062667s
[INFO] 10.244.0.149:44331 - 35590 "A IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.00015689s
[INFO] 10.244.0.149:41614 - 44734 "A IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000063895s
[INFO] 10.244.0.149:41614 - 44930 "AAAA IN ac-kg3i329-shard-00-00.z3jjaj2.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 390 0.000200685s
[INFO] 10.244.0.151:56091 - 53711 "A IN metadata.google.internal. udp 42 false 512" NXDOMAIN qr,rd,ra 117 0.001617145s
[INFO] 10.244.0.151:56091 - 16818 "AAAA IN metadata.google.internal. udp 42 false 512" NXDOMAIN qr,rd,ra 117 0.002779674s
[INFO] 10.244.0.153:38484 - 30213 "AAAA IN shippingservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 160 0.001584084s
[INFO] 10.244.0.153:58992 - 34008 "A IN paymentservice.online-boutique.svc.cluster.local. udp 77 false 1232" NOERROR qr,aa,rd 130 0.001962233s
[INFO] 10.244.0.153:57617 - 1334 "AAAA IN productcatalogservice.online-boutique.svc.cluster.local. udp 84 false 1232" NOERROR qr,aa,rd 166 0.000513462s
[INFO] 10.244.0.153:59082 - 59031 "A IN cartservice.online-boutique.svc.cluster.local. udp 74 false 1232" NOERROR qr,aa,rd 124 0.000459141s
[INFO] 10.244.0.153:57189 - 62084 "A IN shippingservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 132 0.000339622s
[INFO] 10.244.0.153:51086 - 14938 "A IN productcatalogservice.online-boutique.svc.cluster.local. udp 84 false 1232" NOERROR qr,aa,rd 144 0.00028958s
[INFO] 10.244.0.153:58770 - 37530 "AAAA IN cartservice.online-boutique.svc.cluster.local. udp 74 false 1232" NOERROR qr,aa,rd 156 0.000276261s
[INFO] 10.244.0.153:60292 - 34108 "AAAA IN emailservice.online-boutique.svc.cluster.local. udp 75 false 1232" NOERROR qr,aa,rd 157 0.000316419s
[INFO] 10.244.0.153:57279 - 46869 "AAAA IN currencyservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 160 0.000271292s
[INFO] 10.244.0.153:37299 - 57450 "AAAA IN paymentservice.online-boutique.svc.cluster.local. udp 77 false 1232" NOERROR qr,aa,rd 159 0.000076504s
[INFO] 10.244.0.153:40437 - 33604 "A IN emailservice.online-boutique.svc.cluster.local. udp 75 false 1232" NOERROR qr,aa,rd 126 0.000463496s
[INFO] 10.244.0.153:34736 - 25605 "A IN currencyservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 132 0.00026937s
[INFO] 10.244.0.155:57787 - 36600 "A IN adservice.online-boutique.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000828298s
[INFO] 10.244.0.155:58960 - 61990 "A IN currencyservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 132 0.000592604s
[INFO] 10.244.0.155:56844 - 1672 "A IN productcatalogservice.online-boutique.svc.cluster.local. udp 84 false 1232" NOERROR qr,aa,rd 144 0.000643993s
[INFO] 10.244.0.155:36968 - 39829 "A IN cartservice.online-boutique.svc.cluster.local. udp 74 false 1232" NOERROR qr,aa,rd 124 0.000668899s
[INFO] 10.244.0.155:45385 - 14285 "A IN recommendationservice.online-boutique.svc.cluster.local. udp 84 false 1232" NOERROR qr,aa,rd 144 0.001556032s
[INFO] 10.244.0.155:33472 - 19490 "A IN shippingservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 132 0.001708393s
[INFO] 10.244.0.155:57728 - 47284 "A IN checkoutservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 132 0.001724277s
[INFO] 10.244.0.155:50953 - 8454 "AAAA IN cartservice.online-boutique.svc.cluster.local. udp 74 false 1232" NOERROR qr,aa,rd 156 0.000091427s
[INFO] 10.244.0.155:44426 - 63497 "AAAA IN currencyservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 160 0.000079835s
[INFO] 10.244.0.155:54807 - 55644 "AAAA IN productcatalogservice.online-boutique.svc.cluster.local. udp 84 false 1232" NOERROR qr,aa,rd 166 0.000128544s
[INFO] 10.244.0.155:48010 - 50527 "AAAA IN shippingservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 160 0.000059876s
[INFO] 10.244.0.155:34561 - 693 "AAAA IN adservice.online-boutique.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.003992402s
[INFO] 10.244.0.155:56394 - 23799 "AAAA IN recommendationservice.online-boutique.svc.cluster.local. udp 84 false 1232" NOERROR qr,aa,rd 166 0.000218699s
[INFO] 10.244.0.155:47722 - 32809 "AAAA IN checkoutservice.online-boutique.svc.cluster.local. udp 78 false 1232" NOERROR qr,aa,rd 160 0.000079921s


==> describe nodes <==
Name:               multinode-demo
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    department=ecommerce
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=multinode-demo
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=multinode-demo
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_03T17_41_47_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
                    rank=5
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 03 May 2024 17:41:44 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  multinode-demo
  AcquireTime:     <unset>
  RenewTime:       Thu, 30 May 2024 05:53:06 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 30 May 2024 05:51:40 +0000   Mon, 20 May 2024 17:08:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 30 May 2024 05:51:40 +0000   Mon, 20 May 2024 17:08:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 30 May 2024 05:51:40 +0000   Mon, 20 May 2024 17:08:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 30 May 2024 05:51:40 +0000   Mon, 20 May 2024 17:08:56 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.67.2
  Hostname:    multinode-demo
Capacity:
  cpu:                2
  ephemeral-storage:  162406320Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8140760Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  162406320Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8140760Ki
  pods:               110
System Info:
  Machine ID:                 45f40e5bf1a24fdbb560bc62730ac204
  System UUID:                b484bff3-6249-4ac5-b87c-c7fbd40f7cbc
  Boot ID:                    687f3467-9e2f-4e04-a20e-b8eac2451313
  Kernel Version:             5.15.0-67-generic
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (19 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-7db6d8ff4d-66mdv                    100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     26d
  kube-system                 etcd-multinode-demo                         100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         26d
  kube-system                 kindnet-pc4pj                               100m (5%!)(MISSING)     100m (5%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      26d
  kube-system                 kube-apiserver-multinode-demo               250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 kube-controller-manager-multinode-demo      200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 kube-proxy-x522v                            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 kube-scheduler-multinode-demo               100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 metrics-server-7ffbc6d68-qxbpw              100m (5%!)(MISSING)     0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         22d
  kube-system                 metrics-server-c59844bb4-zmch4              100m (5%!)(MISSING)     0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         22d
  kube-system                 storage-provisioner                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26d
  kube-system                 vpa-admission-controller-cb5f8dbc8-grhg2    50m (2%!)(MISSING)      200m (10%!)(MISSING)  200Mi (2%!)(MISSING)       500Mi (6%!)(MISSING)     21d
  kube-system                 vpa-recommender-58886885f-dr5sv             50m (2%!)(MISSING)      200m (10%!)(MISSING)  500Mi (6%!)(MISSING)       1000Mi (12%!)(MISSING)   21d
  kube-system                 vpa-updater-5f44c47fd6-j95b8                50m (2%!)(MISSING)      200m (10%!)(MISSING)  500Mi (6%!)(MISSING)       1000Mi (12%!)(MISSING)   21d
  online-boutique             adservice-5d4f9d9758-zqq9w                  200m (10%!)(MISSING)    300m (15%!)(MISSING)  180Mi (2%!)(MISSING)       300Mi (3%!)(MISSING)     2d13h
  online-boutique             cartservice-79795bd44f-cgckf                200m (10%!)(MISSING)    300m (15%!)(MISSING)  64Mi (0%!)(MISSING)        128Mi (1%!)(MISSING)     2d13h
  online-boutique             checkoutservice-64df5f5d7b-4m8n7            100m (5%!)(MISSING)     200m (10%!)(MISSING)  64Mi (0%!)(MISSING)        128Mi (1%!)(MISSING)     2d13h
  online-boutique             currencyservice-86666df7bb-qzbzf            100m (5%!)(MISSING)     200m (10%!)(MISSING)  64Mi (0%!)(MISSING)        128Mi (1%!)(MISSING)     2d13h
  online-boutique             emailservice-859f88dbd8-b6884               100m (5%!)(MISSING)     200m (10%!)(MISSING)  64Mi (0%!)(MISSING)        128Mi (1%!)(MISSING)     2d13h
  online-boutique             frontend-84c66d58cd-2f26m                   100m (5%!)(MISSING)     200m (10%!)(MISSING)  64Mi (0%!)(MISSING)        128Mi (1%!)(MISSING)     2d13h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                2 (100%!)(MISSING)      2100m (105%!)(MISSING)
  memory             2320Mi (29%!)(MISSING)  3660Mi (46%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[  +0.000004]  show_stack+0x52/0x5c
[  +0.000006]  dump_stack_lvl+0x4a/0x63
[  +0.000003]  dump_stack+0x10/0x16
[  +0.000001]  dump_header+0x53/0x228
[  +0.000004]  oom_kill_process.cold+0xb/0x10
[  +0.000003]  out_of_memory+0x106/0x2e0
[  +0.000007]  mem_cgroup_out_of_memory+0x13f/0x160
[  +0.000003]  try_charge_memcg+0x68e/0x750
[  +0.000003]  charge_memcg+0x45/0xb0
[  +0.000002]  __mem_cgroup_charge+0x2d/0x90
[  +0.000002]  do_anonymous_page+0x114/0x3c0
[  +0.000015]  handle_pte_fault+0x20a/0x240
[  +0.000002]  __handle_mm_fault+0x405/0x6f0
[  +0.000004]  handle_mm_fault+0xd8/0x2c0
[  +0.000002]  do_user_addr_fault+0x1c9/0x670
[  +0.000004]  exc_page_fault+0x77/0x170
[  +0.000005]  asm_exc_page_fault+0x27/0x30
[  +0.000003] RIP: 0033:0x55eb6c4c8e80
[  +0.000003] Code: 84 a7 02 00 00 8b 54 24 0c 31 c0 85 d2 0f 94 c0 89 44 24 08 41 83 fe 02 0f 8f 46 01 00 00 31 c0 48 85 ed 7e 12 0f 1f 44 00 00 <41> c6 04 07 5a 48 01 d8 48 39 c5 7f f3 48 83 3c 24 00 0f 84 fe 01
[  +0.000001] RSP: 002b:00007ffe24c8a3b0 EFLAGS: 00010206
[  +0.000003] RAX: 00000000122c5000 RBX: 0000000000001000 RCX: 00007f352e744bd4
[  +0.000001] RDX: 0000000000000000 RSI: 0000000040001000 RDI: 00007f34ee70f000
[  +0.000001] RBP: 0000000040000000 R08: ffffffffffffffff R09: 0000000000000000
[  +0.000001] R10: 0000000000000022 R11: 0000000000000246 R12: 00007f352e7a12c0
[  +0.000002] R13: 00007f352e7a12c0 R14: 0000000000000002 R15: 00007f34ee70f020
[  +0.000003]  </TASK>
[  +0.001130] Memory cgroup out of memory: Killed process 2460576 (stress) total-vm:1049360kB, anon-rss:297532kB, file-rss:248kB, shmem-rss:0kB, UID:0 pgtables:624kB oom_score_adj:968
[May 5 22:25] stress invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=968
[  +0.000007] CPU: 0 PID: 2463285 Comm: stress Not tainted 5.15.0-67-generic #74-Ubuntu
[  +0.000003] Hardware name: DigitalOcean Droplet/Droplet, BIOS 20171212 12/12/2017
[  +0.000001] Call Trace:
[  +0.000002]  <TASK>
[  +0.000004]  show_stack+0x52/0x5c
[  +0.000006]  dump_stack_lvl+0x4a/0x63
[  +0.000004]  dump_stack+0x10/0x16
[  +0.000001]  dump_header+0x53/0x228
[  +0.000004]  oom_kill_process.cold+0xb/0x10
[  +0.000002]  out_of_memory+0x106/0x2e0
[  +0.000006]  mem_cgroup_out_of_memory+0x13f/0x160
[  +0.000003]  try_charge_memcg+0x68e/0x750
[  +0.000003]  charge_memcg+0x45/0xb0
[  +0.000012]  __mem_cgroup_charge+0x2d/0x90
[  +0.000002]  do_anonymous_page+0x114/0x3c0
[  +0.000004]  handle_pte_fault+0x20a/0x240
[  +0.000003]  __handle_mm_fault+0x405/0x6f0
[  +0.000003]  handle_mm_fault+0xd8/0x2c0
[  +0.000002]  do_user_addr_fault+0x1c9/0x670
[  +0.000005]  exc_page_fault+0x77/0x170
[  +0.000005]  asm_exc_page_fault+0x27/0x30
[  +0.000005] RIP: 0033:0x55b998998e80
[  +0.000004] Code: 84 a7 02 00 00 8b 54 24 0c 31 c0 85 d2 0f 94 c0 89 44 24 08 41 83 fe 02 0f 8f 46 01 00 00 31 c0 48 85 ed 7e 12 0f 1f 44 00 00 <41> c6 04 07 5a 48 01 d8 48 39 c5 7f f3 48 83 3c 24 00 0f 84 fe 01
[  +0.000001] RSP: 002b:00007ffc2aa20e50 EFLAGS: 00010206
[  +0.000003] RAX: 00000000122b3000 RBX: 0000000000001000 RCX: 00007ff1889c4bd4
[  +0.000001] RDX: 0000000000000000 RSI: 0000000040001000 RDI: 00007ff14898f000
[  +0.000002] RBP: 0000000040000000 R08: ffffffffffffffff R09: 0000000000000000
[  +0.000001] R10: 0000000000000022 R11: 0000000000000246 R12: 00007ff188a212c0
[  +0.000001] R13: 00007ff188a212c0 R14: 0000000000000002 R15: 00007ff14898f020
[  +0.000004]  </TASK>
[  +0.000057] Memory cgroup out of memory: Killed process 2463285 (stress) total-vm:1049360kB, anon-rss:297528kB, file-rss:248kB, shmem-rss:0kB, UID:0 pgtables:628kB oom_score_adj:968
[May24 12:50] kauditd_printk_skb: 6 callbacks suppressed


==> etcd [c04ca8e7ae09] <==
{"level":"info","ts":"2024-05-30T04:17:30.465273Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2149670}
{"level":"info","ts":"2024-05-30T04:17:30.474308Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2149670,"took":"8.37074ms","hash":498142510,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1540096,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T04:17:30.474372Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":498142510,"revision":2149670,"compact-revision":2149394}
{"level":"info","ts":"2024-05-30T04:22:30.469736Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2149948}
{"level":"info","ts":"2024-05-30T04:22:30.472167Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2149948,"took":"2.08551ms","hash":405579023,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1536000,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T04:22:30.472208Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":405579023,"revision":2149948,"compact-revision":2149670}
{"level":"info","ts":"2024-05-30T04:27:30.474726Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2150224}
{"level":"info","ts":"2024-05-30T04:27:30.477156Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2150224,"took":"1.808437ms","hash":2553904338,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1536000,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T04:27:30.477252Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2553904338,"revision":2150224,"compact-revision":2149948}
{"level":"info","ts":"2024-05-30T04:32:30.482878Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2150500}
{"level":"info","ts":"2024-05-30T04:32:30.486533Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2150500,"took":"3.103846ms","hash":3823446019,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1544192,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T04:32:30.486582Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3823446019,"revision":2150500,"compact-revision":2150224}
{"level":"info","ts":"2024-05-30T04:37:30.49076Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2150778}
{"level":"info","ts":"2024-05-30T04:37:30.4931Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2150778,"took":"1.953838ms","hash":3602189377,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1540096,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T04:37:30.493143Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3602189377,"revision":2150778,"compact-revision":2150500}
{"level":"info","ts":"2024-05-30T04:42:30.495838Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2151055}
{"level":"info","ts":"2024-05-30T04:42:30.504111Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2151055,"took":"6.939567ms","hash":111483055,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1536000,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T04:42:30.504184Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":111483055,"revision":2151055,"compact-revision":2150778}
{"level":"info","ts":"2024-05-30T04:47:30.500248Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2151332}
{"level":"info","ts":"2024-05-30T04:47:30.50314Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2151332,"took":"2.287635ms","hash":1709004850,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1540096,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T04:47:30.503196Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1709004850,"revision":2151332,"compact-revision":2151055}
{"level":"info","ts":"2024-05-30T04:52:30.50568Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2151609}
{"level":"info","ts":"2024-05-30T04:52:30.509688Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2151609,"took":"3.092592ms","hash":778975644,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1548288,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T04:52:30.509759Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":778975644,"revision":2151609,"compact-revision":2151332}
{"level":"info","ts":"2024-05-30T04:57:30.509975Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2151885}
{"level":"info","ts":"2024-05-30T04:57:30.512549Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2151885,"took":"1.976378ms","hash":1685946758,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1544192,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T04:57:30.512618Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1685946758,"revision":2151885,"compact-revision":2151609}
{"level":"info","ts":"2024-05-30T05:02:30.515521Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2152162}
{"level":"info","ts":"2024-05-30T05:02:30.51931Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2152162,"took":"3.145232ms","hash":1150764426,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1523712,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T05:02:30.51938Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1150764426,"revision":2152162,"compact-revision":2151885}
{"level":"info","ts":"2024-05-30T05:07:30.523529Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2152439}
{"level":"info","ts":"2024-05-30T05:07:30.525825Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2152439,"took":"1.933134ms","hash":1438760784,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1527808,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T05:07:30.525879Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1438760784,"revision":2152439,"compact-revision":2152162}
{"level":"info","ts":"2024-05-30T05:12:30.529408Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2152715}
{"level":"info","ts":"2024-05-30T05:12:30.53224Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2152715,"took":"2.295336ms","hash":2505277076,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1523712,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T05:12:30.532581Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2505277076,"revision":2152715,"compact-revision":2152439}
{"level":"info","ts":"2024-05-30T05:17:30.536611Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2152992}
{"level":"info","ts":"2024-05-30T05:17:30.539875Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2152992,"took":"2.703442ms","hash":1108228925,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1527808,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T05:17:30.539927Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1108228925,"revision":2152992,"compact-revision":2152715}
{"level":"info","ts":"2024-05-30T05:22:30.543447Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2153268}
{"level":"info","ts":"2024-05-30T05:22:30.546556Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2153268,"took":"2.468611ms","hash":3460683707,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1552384,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-05-30T05:22:30.546678Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3460683707,"revision":2153268,"compact-revision":2152992}
{"level":"info","ts":"2024-05-30T05:27:30.547987Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2153546}
{"level":"info","ts":"2024-05-30T05:27:30.550569Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2153546,"took":"1.935372ms","hash":2887384527,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1540096,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-05-30T05:27:30.55077Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2887384527,"revision":2153546,"compact-revision":2153268}
{"level":"info","ts":"2024-05-30T05:32:30.554958Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2153823}
{"level":"info","ts":"2024-05-30T05:32:30.557646Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2153823,"took":"2.095712ms","hash":3114660701,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1552384,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-05-30T05:32:30.557888Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3114660701,"revision":2153823,"compact-revision":2153546}
{"level":"info","ts":"2024-05-30T05:37:30.561837Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2154100}
{"level":"info","ts":"2024-05-30T05:37:30.564414Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2154100,"took":"1.996042ms","hash":1685940859,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1556480,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-05-30T05:37:30.564557Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1685940859,"revision":2154100,"compact-revision":2153823}
{"level":"info","ts":"2024-05-30T05:42:30.568024Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2154377}
{"level":"info","ts":"2024-05-30T05:42:30.571313Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2154377,"took":"2.382819ms","hash":1360743878,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":1552384,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-05-30T05:42:30.571454Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1360743878,"revision":2154377,"compact-revision":2154100}
{"level":"info","ts":"2024-05-30T05:47:30.606554Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2154653}
{"level":"info","ts":"2024-05-30T05:47:30.621666Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2154653,"took":"14.631929ms","hash":4267161307,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":2134016,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-05-30T05:47:30.621733Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4267161307,"revision":2154653,"compact-revision":2154377}
{"level":"info","ts":"2024-05-30T05:52:30.617919Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2155029}
{"level":"info","ts":"2024-05-30T05:52:30.62198Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2155029,"took":"3.29145ms","hash":1046340386,"current-db-size-bytes":6135808,"current-db-size":"6.1 MB","current-db-size-in-use-bytes":2187264,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-05-30T05:52:30.622119Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1046340386,"revision":2155029,"compact-revision":2154653}


==> kernel <==
 05:53:12 up 27 days, 14:00,  0 users,  load average: 0.23, 0.40, 0.36
Linux multinode-demo 5.15.0-67-generic #74-Ubuntu SMP Wed Feb 22 14:14:39 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kindnet [316a57c597c6] <==
I0530 05:48:13.422247       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:48:13.422440       1 main.go:227] handling current node
I0530 05:48:23.429057       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:48:23.429094       1 main.go:227] handling current node
I0530 05:48:33.435764       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:48:33.436302       1 main.go:227] handling current node
I0530 05:48:43.450652       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:48:43.450691       1 main.go:227] handling current node
I0530 05:48:53.455668       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:48:53.455916       1 main.go:227] handling current node
I0530 05:49:03.469815       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:49:03.469845       1 main.go:227] handling current node
I0530 05:49:13.483204       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:49:13.483320       1 main.go:227] handling current node
I0530 05:49:23.496259       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:49:23.496384       1 main.go:227] handling current node
I0530 05:49:33.501826       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:49:33.501946       1 main.go:227] handling current node
I0530 05:49:43.521012       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:49:43.521064       1 main.go:227] handling current node
I0530 05:49:53.526893       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:49:53.527113       1 main.go:227] handling current node
I0530 05:50:03.542997       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:50:03.543049       1 main.go:227] handling current node
I0530 05:50:13.550488       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:50:13.550526       1 main.go:227] handling current node
I0530 05:50:23.563066       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:50:23.563167       1 main.go:227] handling current node
I0530 05:50:33.570469       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:50:33.570724       1 main.go:227] handling current node
I0530 05:50:43.575457       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:50:43.575494       1 main.go:227] handling current node
I0530 05:50:53.589239       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:50:53.589390       1 main.go:227] handling current node
I0530 05:51:03.603561       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:51:03.603774       1 main.go:227] handling current node
I0530 05:51:13.614925       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:51:13.614971       1 main.go:227] handling current node
I0530 05:51:23.625193       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:51:23.625223       1 main.go:227] handling current node
I0530 05:51:33.630772       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:51:33.631036       1 main.go:227] handling current node
I0530 05:51:43.644218       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:51:43.644578       1 main.go:227] handling current node
I0530 05:51:53.653199       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:51:53.653491       1 main.go:227] handling current node
I0530 05:52:03.658640       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:52:03.658919       1 main.go:227] handling current node
I0530 05:52:13.668955       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:52:13.668988       1 main.go:227] handling current node
I0530 05:52:23.682080       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:52:23.682399       1 main.go:227] handling current node
I0530 05:52:33.689187       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:52:33.689471       1 main.go:227] handling current node
I0530 05:52:43.705734       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:52:43.706113       1 main.go:227] handling current node
I0530 05:52:53.717069       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:52:53.717326       1 main.go:227] handling current node
I0530 05:53:03.731980       1 main.go:223] Handling node with IPs: map[192.168.67.2:{}]
I0530 05:53:03.732031       1 main.go:227] handling current node


==> kube-apiserver [c20f0524e550] <==
I0520 17:09:11.140022       1 trace.go:236] Trace[1968163178]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f61186b1-664a-4e21-92c6-3f9363d24865,client:192.168.67.2,api-group:,api-version:v1,name:metrics-server-7ffbc6d68-qxbpw,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/metrics-server-7ffbc6d68-qxbpw,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:GET (20-May-2024 17:09:10.594) (total time: 545ms):
Trace[1968163178]: ---"About to write a response" 532ms (17:09:11.127)
Trace[1968163178]: [545.188832ms] [545.188832ms] END
I0520 17:09:11.231794       1 trace.go:236] Trace[492743069]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4721b74e-09ed-449a-b71a-89aba1e26f2b,client:192.168.67.2,api-group:coordination.k8s.io,api-version:v1,name:multinode-demo,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/multinode-demo,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (20-May-2024 17:09:10.663) (total time: 567ms):
Trace[492743069]: ["GuaranteedUpdate etcd3" audit-id:4721b74e-09ed-449a-b71a-89aba1e26f2b,key:/leases/kube-node-lease/multinode-demo,type:*coordination.Lease,resource:leases.coordination.k8s.io 567ms (17:09:10.664)
Trace[492743069]:  ---"About to Encode" 121ms (17:09:10.785)
Trace[492743069]:  ---"Txn call completed" 406ms (17:09:11.191)]
Trace[492743069]: [567.788059ms] [567.788059ms] END
I0520 17:09:11.277828       1 trace.go:236] Trace[282495893]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.67.2,type:*v1.Endpoints,resource:apiServerIPInfo (20-May-2024 17:09:09.897) (total time: 1378ms):
Trace[282495893]: ---"initial value restored" 687ms (17:09:10.584)
Trace[282495893]: ---"Transaction prepared" 340ms (17:09:10.925)
Trace[282495893]: ---"Txn call completed" 350ms (17:09:11.275)
Trace[282495893]: [1.378683059s] [1.378683059s] END
I0520 17:09:11.303808       1 trace.go:236] Trace[1981192674]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ee5f8320-8204-449d-b224-b1e85483e985,client:192.168.67.2,api-group:apps,api-version:v1,name:vpa-admission-controller,subresource:status,namespace:kube-system,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kube-system/deployments/vpa-admission-controller/status,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:deployment-controller,verb:PUT (20-May-2024 17:09:10.786) (total time: 511ms):
Trace[1981192674]: ["GuaranteedUpdate etcd3" audit-id:ee5f8320-8204-449d-b224-b1e85483e985,key:/deployments/kube-system/vpa-admission-controller,type:*apps.Deployment,resource:deployments.apps 513ms (17:09:10.790)]
Trace[1981192674]: [511.491857ms] [511.491857ms] END
I0520 17:09:11.338288       1 trace.go:236] Trace[418789540]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:bc9ae69b-07f1-45df-b39f-50198a2d5232,client:192.168.67.2,api-group:,api-version:v1,name:etcd-multinode-demo.17d0831292c33e1e,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/etcd-multinode-demo.17d0831292c33e1e,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PATCH (20-May-2024 17:09:10.595) (total time: 720ms):
Trace[418789540]: ["GuaranteedUpdate etcd3" audit-id:bc9ae69b-07f1-45df-b39f-50198a2d5232,key:/events/kube-system/etcd-multinode-demo.17d0831292c33e1e,type:*core.Event,resource:events 742ms (17:09:10.595)
Trace[418789540]:  ---"initial value restored" 447ms (17:09:11.043)
Trace[418789540]:  ---"About to Encode" 154ms (17:09:11.197)
Trace[418789540]:  ---"Txn call completed" 117ms (17:09:11.315)]
Trace[418789540]: ---"About to check admission control" 110ms (17:09:11.153)
Trace[418789540]: ---"Object stored in database" 161ms (17:09:11.315)
Trace[418789540]: [720.658846ms] [720.658846ms] END
W0520 17:09:27.711516       1 handler_proxy.go:93] no RequestInfo found in the context
E0520 17:09:27.711859       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0520 17:09:27.725091       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1: Get "https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.97.180.197:443: connect: connection refused
E0520 17:09:27.748711       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1: Get "https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.97.180.197:443: connect: connection refused
E0520 17:09:27.792979       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1: Get "https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.97.180.197:443: connect: connection refused
E0520 17:09:27.804199       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1: Get "https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.97.180.197:443: connect: connection refused
E0520 17:09:27.845405       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1: Get "https://10.97.180.197:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.97.180.197:443: connect: connection refused
I0520 17:09:28.466451       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0520 22:23:29.431487       1 alloc.go:330] "allocated clusterIPs" service="devshack-ns/backend-service" clusterIPs={"IPv4":"10.108.167.192"}
I0520 22:23:29.656137       1 alloc.go:330] "allocated clusterIPs" service="devshack-ns/frontend-service" clusterIPs={"IPv4":"10.105.200.41"}
I0521 07:37:42.292968       1 alloc.go:330] "allocated clusterIPs" service="devshack-ns/backend-service" clusterIPs={"IPv4":"10.102.145.119"}
I0521 07:37:42.492596       1 alloc.go:330] "allocated clusterIPs" service="devshack-ns/frontend-service" clusterIPs={"IPv4":"10.104.18.71"}
I0521 07:37:42.558755       1 alloc.go:330] "allocated clusterIPs" service="devshack-ns/mongo" clusterIPs={"IPv4":"10.102.192.107"}
I0521 08:01:26.229467       1 alloc.go:330] "allocated clusterIPs" service="devshack-ns/backend-service" clusterIPs={"IPv4":"10.96.109.134"}
I0521 08:01:26.375926       1 alloc.go:330] "allocated clusterIPs" service="devshack-ns/frontend-service" clusterIPs={"IPv4":"10.99.176.99"}
I0523 22:02:48.154285       1 trace.go:236] Trace[285056984]: "Get" accept:application/json, */*,audit-id:2fce4482-d755-4822-a4ab-675e7bde13c6,client:192.168.67.1,api-group:,api-version:v1,name:etcd-multinode-demo,subresource:log,namespace:kube-system,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/etcd-multinode-demo/log,user-agent:kubectl/v1.29.0 (linux/amd64) kubernetes/3034fd4,verb:CONNECT (23-May-2024 22:02:47.500) (total time: 652ms):
Trace[285056984]: ---"Writing http response done" 650ms (22:02:48.152)
Trace[285056984]: [652.476171ms] [652.476171ms] END
I0526 22:13:55.607237       1 alloc.go:330] "allocated clusterIPs" service="xxx/backend-service" clusterIPs={"IPv4":"10.100.136.110"}
I0526 22:14:30.011040       1 alloc.go:330] "allocated clusterIPs" service="xxx/frontend-service" clusterIPs={"IPv4":"10.103.102.42"}
I0527 16:43:22.921870       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/adservice" clusterIPs={"IPv4":"10.101.36.19"}
I0527 16:43:23.120362       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/cartservice" clusterIPs={"IPv4":"10.106.20.58"}
I0527 16:43:23.209477       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/checkoutservice" clusterIPs={"IPv4":"10.103.123.239"}
I0527 16:43:23.403301       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/currencyservice" clusterIPs={"IPv4":"10.110.159.142"}
I0527 16:43:23.703623       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/emailservice" clusterIPs={"IPv4":"10.109.29.70"}
I0527 16:43:23.942203       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/frontend" clusterIPs={"IPv4":"10.103.71.251"}
I0527 16:43:24.222047       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/frontend-external" clusterIPs={"IPv4":"10.110.167.151"}
I0527 16:43:24.432353       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/paymentservice" clusterIPs={"IPv4":"10.103.0.205"}
I0527 16:43:24.932693       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/productcatalogservice" clusterIPs={"IPv4":"10.103.138.80"}
I0527 16:43:25.079917       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/recommendationservice" clusterIPs={"IPv4":"10.96.200.218"}
I0527 16:43:25.149401       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/redis-cart" clusterIPs={"IPv4":"10.109.219.18"}
I0527 16:43:25.272266       1 alloc.go:330] "allocated clusterIPs" service="online-boutique/shippingservice" clusterIPs={"IPv4":"10.103.114.214"}
I0530 05:46:01.817221       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.111.65.74"}
I0530 05:46:01.862373       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.107.11.131"}
I0530 05:46:01.929065       1 controller.go:615] quota admission added evaluator for: jobs.batch


==> kube-controller-manager [4579c55d3358] <==
I0527 16:43:36.805202       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/checkoutservice-64df5f5d7b" duration="87.053¬µs"
I0527 16:43:52.102924       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/currencyservice-86666df7bb" duration="1.393137ms"
I0527 16:43:55.183586       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/frontend-84c66d58cd" duration="56.79¬µs"
I0527 16:44:03.492119       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/cartservice-79795bd44f" duration="25.908086ms"
I0527 16:44:03.492235       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/cartservice-79795bd44f" duration="85.504¬µs"
I0527 16:44:03.791875       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/currencyservice-86666df7bb" duration="24.618099ms"
I0527 16:44:03.792213       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/currencyservice-86666df7bb" duration="58.342¬µs"
I0527 16:44:04.371785       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/frontend-84c66d58cd" duration="33.042095ms"
I0527 16:44:04.380770       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/frontend-84c66d58cd" duration="133.665¬µs"
I0527 16:44:05.421369       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/emailservice-859f88dbd8" duration="99.774¬µs"
I0527 16:44:08.589026       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/adservice-5d4f9d9758" duration="14.939841ms"
I0527 16:44:08.589353       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/adservice-5d4f9d9758" duration="284.288¬µs"
I0527 16:44:09.035584       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/emailservice-859f88dbd8" duration="24.12638ms"
I0527 16:44:09.036367       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/emailservice-859f88dbd8" duration="94.475¬µs"
I0529 01:40:47.973996       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/cartservice-79795bd44f" duration="166.451¬µs"
I0529 01:40:47.973996       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/currencyservice-86666df7bb" duration="116.935¬µs"
I0529 01:40:47.974111       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/loadgenerator-6cffd7784d" duration="60.474¬µs"
I0529 01:40:47.974490       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="51.644¬µs"
I0529 01:40:47.974486       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/redis-cart-7ff8f4d6ff" duration="50.238¬µs"
I0529 01:40:47.974608       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/vpa-updater-5f44c47fd6" duration="96.141¬µs"
I0529 01:40:47.974730       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7ffbc6d68" duration="228.472¬µs"
I0529 01:40:47.974861       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/emailservice-859f88dbd8" duration="30.862¬µs"
I0529 01:40:47.974809       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/checkoutservice-64df5f5d7b" duration="171.922¬µs"
I0529 01:40:47.974992       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/productcatalogservice-85b6564bc8" duration="28.909¬µs"
I0529 01:40:47.975085       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="58.52¬µs"
I0529 01:40:47.975177       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/vpa-admission-controller-cb5f8dbc8" duration="74.406¬µs"
I0529 01:40:47.975083       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/frontend-84c66d58cd" duration="151.909¬µs"
I0529 01:40:47.975250       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/paymentservice-59c998b848" duration="38.674¬µs"
I0529 01:40:47.975312       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/vpa-recommender-58886885f" duration="145.495¬µs"
I0529 01:40:47.975409       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/recommendationservice-799f55db88" duration="97.101¬µs"
I0529 01:40:47.975486       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/adservice-5d4f9d9758" duration="26.386¬µs"
I0529 01:40:47.975545       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="online-boutique/shippingservice-7dd9f997c8" duration="94.644¬µs"
I0530 05:46:01.952109       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:01.975571       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:02.079086       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:02.079152       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:02.110238       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="121.785736ms"
I0530 05:46:02.124380       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:02.124790       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:02.134632       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:02.148585       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:02.195846       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="85.393166ms"
I0530 05:46:02.213421       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:02.244702       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="48.763471ms"
I0530 05:46:02.245230       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-84df5799c" duration="158.812¬µs"
I0530 05:46:02.245445       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:05.254032       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:05.288666       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:06.316302       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:06.492695       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:07.371202       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:07.423121       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:07.501772       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:07.521004       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:07.529866       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I0530 05:46:07.662623       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:08.445319       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:08.467132       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:08.482323       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I0530 05:46:08.485948       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"


==> kube-proxy [466cc02ba54d] <==
I0503 17:42:02.086118       1 server_linux.go:69] "Using iptables proxy"
I0503 17:42:02.100386       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.67.2"]
I0503 17:42:02.139120       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0503 17:42:02.139172       1 server_linux.go:165] "Using iptables Proxier"
I0503 17:42:02.141245       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0503 17:42:02.141270       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0503 17:42:02.141319       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0503 17:42:02.141608       1 server.go:872] "Version info" version="v1.30.0"
I0503 17:42:02.141647       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0503 17:42:02.142917       1 config.go:192] "Starting service config controller"
I0503 17:42:02.143098       1 shared_informer.go:313] Waiting for caches to sync for service config
I0503 17:42:02.143143       1 config.go:101] "Starting endpoint slice config controller"
I0503 17:42:02.143147       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0503 17:42:02.143697       1 config.go:319] "Starting node config controller"
I0503 17:42:02.143708       1 shared_informer.go:313] Waiting for caches to sync for node config
I0503 17:42:02.244443       1 shared_informer.go:320] Caches are synced for service config
I0503 17:42:02.250403       1 shared_informer.go:320] Caches are synced for node config
I0503 17:42:02.258561       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0520 17:08:12.970352       1 trace.go:236] Trace[1515812313]: "iptables ChainExists" (20-May-2024 17:08:03.424) (total time: 6003ms):
Trace[1515812313]: [6.003593608s] [6.003593608s] END
I0520 17:08:41.939439       1 trace.go:236] Trace[1407814652]: "iptables ChainExists" (20-May-2024 17:08:32.346) (total time: 9593ms):
Trace[1407814652]: [9.593210642s] [9.593210642s] END
I0520 17:08:41.993521       1 trace.go:236] Trace[325327900]: "iptables ChainExists" (20-May-2024 17:08:33.152) (total time: 8840ms):
Trace[325327900]: [8.840652262s] [8.840652262s] END
I0520 17:08:47.895926       1 trace.go:236] Trace[1596832304]: "iptables save" (20-May-2024 17:08:44.871) (total time: 3024ms):
Trace[1596832304]: [3.024484179s] [3.024484179s] END


==> kube-scheduler [9389a26750cc] <==
E0503 17:41:44.132487       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0503 17:41:44.132615       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0503 17:41:44.133019       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0503 17:41:44.131491       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0503 17:41:44.131575       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0503 17:41:44.131657       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0503 17:41:44.131977       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0503 17:41:44.132185       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0503 17:41:44.132284       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0503 17:41:44.132345       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0503 17:41:44.132386       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0503 17:41:44.132416       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0503 17:41:44.131307       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0503 17:41:44.131408       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0503 17:41:44.134375       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0503 17:41:44.134414       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0503 17:41:44.134423       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0503 17:41:44.134430       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0503 17:41:44.134845       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0503 17:41:44.134945       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0503 17:41:44.134957       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0503 17:41:44.134963       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0503 17:41:44.135075       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0503 17:41:44.135083       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0503 17:41:44.136175       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0503 17:41:45.072048       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0503 17:41:45.072167       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0503 17:41:45.188587       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0503 17:41:45.188623       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0503 17:41:45.264789       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0503 17:41:45.264831       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0503 17:41:45.295482       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0503 17:41:45.295693       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0503 17:41:45.301876       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0503 17:41:45.301916       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0503 17:41:45.371019       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0503 17:41:45.371057       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0503 17:41:45.634938       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0503 17:41:45.635140       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0503 17:41:48.617657       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0513 09:15:31.946814       1 framework.go:1238] "Plugin failed" err="binding volumes: failed to check provisioning pvc: could not find v1.PersistentVolumeClaim \"default/mongo-volume-mongo-0\"" plugin="VolumeBinding" pod="default/mongo-0" node="multinode-demo"
E0513 09:15:31.947294       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreBind plugin \"VolumeBinding\": binding volumes: failed to check provisioning pvc: could not find v1.PersistentVolumeClaim \"default/mongo-volume-mongo-0\"" pod="default/mongo-0"
E0513 09:16:28.300388       1 framework.go:1238] "Plugin failed" err="binding volumes: failed to check provisioning pvc: could not find v1.PersistentVolumeClaim \"default/mongo-volume-mongo-0\"" plugin="VolumeBinding" pod="default/mongo-0" node="multinode-demo"
E0513 09:16:28.300482       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreBind plugin \"VolumeBinding\": binding volumes: failed to check provisioning pvc: could not find v1.PersistentVolumeClaim \"default/mongo-volume-mongo-0\"" pod="default/mongo-0"
E0513 09:19:28.332137       1 framework.go:1238] "Plugin failed" err="binding volumes: failed to check provisioning pvc: could not find v1.PersistentVolumeClaim \"default/mongo-volume-mongo-0\"" plugin="VolumeBinding" pod="default/mongo-0" node="multinode-demo"
E0513 09:19:28.332613       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreBind plugin \"VolumeBinding\": binding volumes: failed to check provisioning pvc: could not find v1.PersistentVolumeClaim \"default/mongo-volume-mongo-0\"" pod="default/mongo-0"
E0513 09:20:12.457231       1 framework.go:1238] "Plugin failed" err="binding volumes: pod does not exist any more: pod \"mongo-0\" not found" plugin="VolumeBinding" pod="default/mongo-0" node="multinode-demo"
E0513 09:20:12.457506       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreBind plugin \"VolumeBinding\": binding volumes: pod does not exist any more: pod \"mongo-0\" not found" pod="default/mongo-0"
I0513 09:20:12.457624       1 schedule_one.go:1055] "Pod doesn't exist in informer cache" pod="default/mongo-0" err="pod \"mongo-0\" not found"
E0513 09:20:12.462011       1 schedule_one.go:1097] "Error updating pod" err="pods \"mongo-0\" not found" pod="default/mongo-0"
E0513 09:41:02.015929       1 framework.go:1238] "Plugin failed" err="binding volumes: context deadline exceeded" plugin="VolumeBinding" pod="default/mongo-0" node="multinode-demo"
E0513 09:41:02.016036       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreBind plugin \"VolumeBinding\": binding volumes: context deadline exceeded" pod="default/mongo-0"
E0513 09:46:47.243212       1 framework.go:1238] "Plugin failed" err="binding volumes: pod does not exist any more: pod \"mongo-0\" not found" plugin="VolumeBinding" pod="default/mongo-0" node="multinode-demo"
E0513 09:46:47.243498       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreBind plugin \"VolumeBinding\": binding volumes: pod does not exist any more: pod \"mongo-0\" not found" pod="default/mongo-0"
I0513 09:46:47.243653       1 schedule_one.go:1055] "Pod doesn't exist in informer cache" pod="default/mongo-0" err="pod \"mongo-0\" not found"
E0513 09:46:47.249068       1 schedule_one.go:1097] "Error updating pod" err="pods \"mongo-0\" not found" pod="default/mongo-0"
E0513 09:51:34.697337       1 framework.go:1238] "Plugin failed" err="binding volumes: pod does not exist any more: pod \"mongo-0\" not found" plugin="VolumeBinding" pod="default/mongo-0" node="multinode-demo"
E0513 09:51:34.697445       1 schedule_one.go:1048] "Error scheduling pod; retrying" err="running PreBind plugin \"VolumeBinding\": binding volumes: pod does not exist any more: pod \"mongo-0\" not found" pod="default/mongo-0"
I0513 09:51:34.697468       1 schedule_one.go:1055] "Pod doesn't exist in informer cache" pod="default/mongo-0" err="pod \"mongo-0\" not found"
E0513 09:51:34.703608       1 schedule_one.go:1097] "Error updating pod" err="pods \"mongo-0\" not found" pod="default/mongo-0"


==> kubelet <==
May 30 05:52:17 multinode-demo kubelet[2011]: E0530 05:52:17.787268    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log\": failed to reopen container log \"95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86" path="/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log" currentSize=37341669 maxSize=10485760
May 30 05:52:17 multinode-demo kubelet[2011]: E0530 05:52:17.795333    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb"
May 30 05:52:17 multinode-demo kubelet[2011]: E0530 05:52:17.795509    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log\": failed to reopen container log \"656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb" path="/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log" currentSize=50914278 maxSize=10485760
May 30 05:52:17 multinode-demo kubelet[2011]: E0530 05:52:17.799181    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24"
May 30 05:52:17 multinode-demo kubelet[2011]: E0530 05:52:17.799290    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log\": failed to reopen container log \"068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24" path="/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log" currentSize=20841346 maxSize=10485760
May 30 05:52:17 multinode-demo kubelet[2011]: E0530 05:52:17.803911    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526"
May 30 05:52:17 multinode-demo kubelet[2011]: E0530 05:52:17.803988    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log\": failed to reopen container log \"316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526" path="/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log" currentSize=98122227 maxSize=10485760
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.766878    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620"
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.766977    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log\": failed to reopen container log \"70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620" path="/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log" currentSize=31897733 maxSize=10485760
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.788282    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86"
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.788365    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log\": failed to reopen container log \"95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86" path="/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log" currentSize=37341860 maxSize=10485760
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.798604    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb"
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.798691    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log\": failed to reopen container log \"656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb" path="/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log" currentSize=50917278 maxSize=10485760
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.803086    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24"
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.803188    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log\": failed to reopen container log \"068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24" path="/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log" currentSize=20841488 maxSize=10485760
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.809453    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526"
May 30 05:52:27 multinode-demo kubelet[2011]: E0530 05:52:27.809531    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log\": failed to reopen container log \"316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526" path="/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log" currentSize=98122520 maxSize=10485760
May 30 05:52:28 multinode-demo kubelet[2011]: I0530 05:52:28.274797    2011 log.go:245] http: TLS handshake error from 10.244.0.54:44906: remote error: tls: bad certificate
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.778602    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620"
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.778735    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log\": failed to reopen container log \"70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620" path="/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log" currentSize=31899182 maxSize=10485760
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.800542    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86"
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.800648    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log\": failed to reopen container log \"95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86" path="/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log" currentSize=37342596 maxSize=10485760
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.806934    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb"
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.807033    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log\": failed to reopen container log \"656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb" path="/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log" currentSize=50917278 maxSize=10485760
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.810962    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24"
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.811083    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log\": failed to reopen container log \"068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24" path="/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log" currentSize=20841488 maxSize=10485760
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.819489    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526"
May 30 05:52:37 multinode-demo kubelet[2011]: E0530 05:52:37.819657    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log\": failed to reopen container log \"316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526" path="/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log" currentSize=98122813 maxSize=10485760
May 30 05:52:43 multinode-demo kubelet[2011]: I0530 05:52:43.273634    2011 log.go:245] http: TLS handshake error from 10.244.0.54:39550: remote error: tls: bad certificate
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.786579    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620"
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.786692    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log\": failed to reopen container log \"70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620" path="/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log" currentSize=31900631 maxSize=10485760
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.809643    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86"
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.809748    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log\": failed to reopen container log \"95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86" path="/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log" currentSize=37343143 maxSize=10485760
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.817608    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb"
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.817713    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log\": failed to reopen container log \"656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb" path="/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log" currentSize=50917278 maxSize=10485760
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.821860    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24"
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.821987    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log\": failed to reopen container log \"068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24" path="/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log" currentSize=20841488 maxSize=10485760
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.831002    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526"
May 30 05:52:47 multinode-demo kubelet[2011]: E0530 05:52:47.831121    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log\": failed to reopen container log \"316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526" path="/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log" currentSize=98123106 maxSize=10485760
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.793658    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620"
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.794360    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log\": failed to reopen container log \"70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620" path="/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log" currentSize=31902079 maxSize=10485760
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.814076    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86"
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.814182    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log\": failed to reopen container log \"95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86" path="/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log" currentSize=37343334 maxSize=10485760
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.824908    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb"
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.824979    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log\": failed to reopen container log \"656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb" path="/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log" currentSize=50917546 maxSize=10485760
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.828895    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24"
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.828988    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log\": failed to reopen container log \"068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24" path="/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log" currentSize=20841488 maxSize=10485760
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.834270    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526"
May 30 05:52:57 multinode-demo kubelet[2011]: E0530 05:52:57.834347    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log\": failed to reopen container log \"316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526" path="/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log" currentSize=98123399 maxSize=10485760
May 30 05:52:58 multinode-demo kubelet[2011]: I0530 05:52:58.266991    2011 log.go:245] http: TLS handshake error from 10.244.0.54:37900: remote error: tls: bad certificate
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.804168    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620"
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.804800    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log\": failed to reopen container log \"70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="70637674be42090aeaef3edab479b20a9479672d3d94cadeee46995c21dd3620" path="/var/log/pods/online-boutique_frontend-84c66d58cd-2f26m_92775c89-8e8b-499c-bf25-de0f616413fb/server/0.log" currentSize=31903529 maxSize=10485760
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.831135    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86"
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.831253    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log\": failed to reopen container log \"95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="95088da314c968f3dfe25f087ddebdd93889a6e58800a166472ab50b8c88cf86" path="/var/log/pods/kube-system_metrics-server-7ffbc6d68-qxbpw_15b06ac9-7710-436b-8cb1-33bf2bd096f4/metrics-server/1.log" currentSize=37343881 maxSize=10485760
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.838888    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb"
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.838996    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log\": failed to reopen container log \"656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="656e04ec6bd33dfe72d0348e6916826cd923df44776666b8f2bc1d87c17eafbb" path="/var/log/pods/kube-system_vpa-recommender-58886885f-dr5sv_86408988-f0be-4dee-b235-fdb600dd416d/recommender/0.log" currentSize=50917826 maxSize=10485760
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.845145    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24"
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.845278    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log\": failed to reopen container log \"068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="068a4ca52f0cbd39c72bbee9f0dfa5ed3d2bb4573d227acfb5736e9f69053b24" path="/var/log/pods/kube-system_vpa-updater-5f44c47fd6-j95b8_28823fbe-db34-4d7b-b3de-ab3f38dfb920/updater/0.log" currentSize=20841738 maxSize=10485760
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.857011    2011 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526"
May 30 05:53:07 multinode-demo kubelet[2011]: E0530 05:53:07.857160    2011 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log\": failed to reopen container log \"316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="316a57c597c61266d9501a8f09f6251c5c0289ef7c9dd965e4be680fb7c53526" path="/var/log/pods/kube-system_kindnet-pc4pj_a2014eb0-0334-4882-94a7-8eabdf64d585/kindnet-cni/0.log" currentSize=98123691 maxSize=10485760

